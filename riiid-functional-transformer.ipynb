{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"riiid-functional-transformer.ipynb","provenance":[{"file_id":"1Haitsb53tj6DxOnzcPBOwbUb_FNeU535","timestamp":1605517993325},{"file_id":"1FIdr7MNGMsOyf6FN2GqwAW_ihg2lC-8l","timestamp":1605382579448},{"file_id":"1olW-HkzoRg8LMM9daj0ZZjCeWZvSZWU4","timestamp":1605105637898},{"file_id":"1GkwSSVnpRXmzEkRort7lR-XgqvZEeOiV","timestamp":1604925749500}],"collapsed_sections":[],"authorship_tag":"ABX9TyNHr/k8E/jnNPXU/ozOcnHH"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"-lO7CgeYAhbA","trusted":true},"source":["def get_angles(pos, i, d_model):\n","  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n","  return pos * angle_rates"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ofWR6tgqBUd0","trusted":true},"source":["def positional_encoding(position, d_model):\n","  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n","                          np.arange(d_model)[np.newaxis, :],\n","                          d_model)\n","  \n","  # apply sin to even indices in the array; 2i\n","  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n","  \n","  # apply cos to odd indices in the array; 2i+1\n","  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n","    \n","  pos_encoding = angle_rads[np.newaxis, ...]\n","    \n","  return tf.cast(pos_encoding, dtype=tf.float32)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U2i8-e1s8ti9","trusted":true},"source":["def create_padding_mask(seq, pad_value=0):\n","  seq = tf.cast(tf.math.equal(seq, pad_value), tf.float32)\n","  \n","  # add extra dimensions to add the padding\n","  # to the attention logits.\n","  return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dVxS8OPI9uI0","trusted":true},"source":["def create_look_ahead_mask(size):\n","  mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n","  return mask  # (seq_len, seq_len)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xOia3dfJJ51C","executionInfo":{"status":"ok","timestamp":1606590068581,"user_tz":-60,"elapsed":683,"user":{"displayName":"Rafi Hai","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiwvzDEpna2dou3v4PeB26xw2-OQQG3tqufXYs5Ag=s64","userId":"15432675270353488986"}}},"source":["# Tensorflow create task mask\n","def create_task_mask_tf(seq):\n","  seq = tf.cast(tf.math.equal(seq, 1), tf.float32)\n","  mask = tf.map_fn(get_task_one_mask_tf, elems=seq)\n","  mask = tf.expand_dims(mask, 1) # make compatible with padding mask\n","  return mask"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"8o0uECdYJ6v9"},"source":["def get_task_one_mask_tf(A):\n","  N = A.shape[0]\n","  column = tf.reshape((A>0), (N,1))\n","  mask = tf.ones((N, N), dtype=tf.bool)\n","  mask = tf.where(column, False, tf.math.logical_not(tf.linalg.band_part(mask, 0, -1)))\n","  mask = tf.cumsum(tf.cast(mask, dtype=tf.float32))\n","  # mask = tf.cast(tf.cumsum(tf.cast(mask, dtype=tf.int8)), dtype=tf.bool)\n","  mask = tf.cast(mask, dtype=tf.bool)\n","  A = tf.expand_dims(A, axis=0)\n","  B = tf.where(mask, 0, tf.tile(A, (N,1)))\n","  B = tf.cast(B, tf.float32)\n","  return B"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kaU5BO82iWgO"},"source":["def create_masks(inp, pad_value): # Look-ahead masking needed for both encoder & decoder\n","    look_ahead_mask = create_look_ahead_mask(tf.shape(inp)[1])\n","    padding_mask = create_padding_mask(inp, pad_value=pad_value)\n","    combined_mask = tf.maximum(padding_mask, look_ahead_mask) # whether there is pad or mask\n","    return combined_mask"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LazzUq3bJ5SH","trusted":true},"source":["def scaled_dot_product_attention(q, k, v, mask):\n","  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n","  \n","  # scale matmul_qk\n","  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n","  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n","\n","  # add the mask to the scaled tensor.\n","  if mask is not None:\n","    scaled_attention_logits += (mask * -1e9)  \n","\n","  # softmax is normalized on the last axis (seq_len_k) so that the scores\n","  # add up to 1.\n","  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n","\n","  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n","\n","  return output, attention_weights"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BSV3PPKsYecw","trusted":true,"colab":{"base_uri":"https://localhost:8080/","height":230},"executionInfo":{"status":"error","timestamp":1605989220550,"user_tz":-60,"elapsed":1176,"user":{"displayName":"Rafi Hai","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiwvzDEpna2dou3v4PeB26xw2-OQQG3tqufXYs5Ag=s64","userId":"15432675270353488986"}},"outputId":"8e153894-d32e-43d4-f0cc-cddba66810c7"},"source":["class MultiHeadAttention(tf.keras.layers.Layer):\n","  def __init__(self, d_model, num_heads):\n","    super(MultiHeadAttention, self).__init__()\n","    self.num_heads = num_heads\n","    self.d_model = d_model\n","    \n","    assert d_model % self.num_heads == 0\n","    \n","    self.depth = d_model // self.num_heads\n","    \n","    self.wq = tf.keras.layers.Dense(d_model)\n","    self.wk = tf.keras.layers.Dense(d_model)\n","    self.wv = tf.keras.layers.Dense(d_model)\n","    \n","    self.dense = tf.keras.layers.Dense(d_model)\n","        \n","  def split_heads(self, x, batch_size):\n","    \"\"\"Split the last dimension into (num_heads, depth).\n","    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n","    \"\"\"\n","    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n","    return tf.transpose(x, perm=[0, 2, 1, 3])\n","    \n","  def call(self, v, k, q, mask):\n","    batch_size = tf.shape(q)[0]\n","    \n","    q = self.wq(q)  # (batch_size, seq_len, d_model)\n","    k = self.wk(k)  # (batch_size, seq_len, d_model)\n","    v = self.wv(v)  # (batch_size, seq_len, d_model)\n","    \n","    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n","    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n","    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n","    \n","    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n","    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n","    scaled_attention, attention_weights = scaled_dot_product_attention(\n","        q, k, v, mask)\n","    \n","    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n","\n","    concat_attention = tf.reshape(scaled_attention, \n","                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n","\n","    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n","        \n","    return output, attention_weights"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-310c98019c19>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mMultiHeadAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMultiHeadAttention\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_heads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"]}]},{"cell_type":"code","metadata":{"id":"ET7xLt0yCT6Z","trusted":true},"source":["def point_wise_feed_forward_network(d_model, dff):\n","  return tf.keras.Sequential([\n","      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n","      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n","  ])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ncyS-Ms3i2x_","trusted":true},"source":["class EncoderLayer(tf.keras.layers.Layer):\n","  def __init__(self, d_model, num_heads, dff, rate=0.1):\n","    super(EncoderLayer, self).__init__()\n","\n","    self.mha = MultiHeadAttention(d_model, num_heads)\n","    self.ffn = point_wise_feed_forward_network(d_model, dff)\n","\n","    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","    \n","    self.dropout1 = tf.keras.layers.Dropout(rate)\n","    self.dropout2 = tf.keras.layers.Dropout(rate)\n","    \n","  def call(self, x, mask):\n","    attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n","    attn_output = self.dropout1(attn_output)\n","    out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n","    \n","    ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n","    ffn_output = self.dropout2(ffn_output)\n","    out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n","    return out2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9SoX0-vd1hue","trusted":true},"source":["class DecoderLayer(tf.keras.layers.Layer):\n","  def __init__(self, d_model, num_heads, dff, rate=0.1):\n","    super(DecoderLayer, self).__init__()\n","\n","    self.mha1 = MultiHeadAttention(d_model, num_heads)\n","    self.mha2 = MultiHeadAttention(d_model, num_heads)\n","\n","    self.ffn = point_wise_feed_forward_network(d_model, dff)\n"," \n","    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","    self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","    \n","    self.dropout1 = tf.keras.layers.Dropout(rate)\n","    self.dropout2 = tf.keras.layers.Dropout(rate)\n","    self.dropout3 = tf.keras.layers.Dropout(rate)\n","    \n","    \n","  def call(self, x, enc_output, mask):\n","    # enc_output.shape == (batch_size, input_seq_len, d_model)\n","\n","    attn1, attn_weights_block1 = self.mha1(x, x, x, mask)  # (batch_size, target_seq_len, d_model)\n","    attn1 = self.dropout1(attn1)\n","    out1 = self.layernorm1(attn1 + x)\n","    \n","    attn2, attn_weights_block2 = self.mha2(\n","        enc_output, enc_output, out1, mask)  # (batch_size, target_seq_len, d_model)\n","    attn2 = self.dropout2(attn2)\n","    out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n","    \n","    ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n","    ffn_output = self.dropout3(ffn_output)\n","    out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n","    \n","    return out3"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RA6MmmCZZzxf"},"source":["def create_model(conf): # Functional model without list comprehension\n","  input = tf.keras.Input(shape=(conf[\"window_size\"], conf[\"n_features\"]), name = \"input\")\n","  seq_len = tf.shape(input)[1]\n","  # mask = create_masks(input[:,:,0], pad_value = conf[\"padding_values\"][0]) # input 0 = E, input 1 = r\n","  mask = create_final_mask(input, pad_value = conf[\"padding_values\"][0]) # input 0 = E, input 1 = r\n","\n","  ###### Encoder ###########\n","  x = 0 # will be casted to sum first embedding\n","  \n","  # Embedding layers\n","  for i in conf[\"enc_emb\"]:\n","    size = conf[\"vocab_sizes\"][i]\n","    embedding = tf.keras.layers.Embedding(int(size), conf[\"d_model\"])\n","    x += embedding(input[:,:,i]) # TODO: Multiply? Remove x=0?\n","  \n","  # Dense layers\n","  for i in conf[\"enc_dense\"]:\n","    dense = tf.keras.layers.Dense(conf[\"d_model\"])\n","    dense_out = dense(input[:,:,i])\n","    dense_out = tf.expand_dims(dense_out, axis=1)\n","    x += dense_out\n","  \n","  # Pos encoding\n","  pos_encoding = positional_encoding(conf[\"window_size\"], conf[\"d_model\"])\n","  pos_encoding = pos_encoding[:, :seq_len, :]\n","  x += pos_encoding\n","\n","  for i in range(conf[\"enc_num_layers\"]):\n","      enc_layer = EncoderLayer(conf[\"d_model\"], conf[\"num_heads\"], conf[\"dff\"], conf[\"dropout_rate\"])\n","      x = enc_layer(x, mask)\n","      out = x\n","  \n","  ####### Decoder #######\n","  y = 0 # will be casted to sum first embedding\n","  \n","  # Embedding layers\n","  for i in conf[\"dec_emb\"]:\n","    size = conf[\"vocab_sizes\"][i]\n","    embedding = tf.keras.layers.Embedding(int(size), conf[\"d_model\"])\n","    y += embedding(input[:,:,i]) # TODO: Multiply? Remove x=0?\n","\n","  # Dense layers\n","  for i in conf[\"dec_dense\"]:\n","    dense = tf.keras.layers.Dense(conf[\"d_model\"])\n","    dense_out = dense(input[:,:,i])\n","    dense_out = tf.expand_dims(dense_out, axis=1)\n","    y += dense_out\n","\n","  # Pos encoding\n","  y += pos_encoding\n","\n","  for i in range(conf[\"dec_num_layers\"]):\n","    dec_layer = DecoderLayer(conf[\"d_model\"], conf[\"num_heads\"], conf[\"dff\"], conf[\"dropout_rate\"])\n","    y = dec_layer(y, x, mask)\n","    out = y\n","  \n","  output = tf.keras.layers.Dense(3)(out) # final layer\n","\n","  # Instantiate an end-to-end model\n","  model = tf.keras.Model(input, output)\n","  return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8CR_EnHhHQB-","executionInfo":{"status":"ok","timestamp":1606571345179,"user_tz":-60,"elapsed":1002,"user":{"displayName":"Rafi Hai","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiwvzDEpna2dou3v4PeB26xw2-OQQG3tqufXYs5Ag=s64","userId":"15432675270353488986"}}},"source":["def create_model_seperate_input(conf):\n","  input = [tf.keras.Input(shape=conf[\"window_size\"], name=conf[\"features\"][i]) for i in range(conf[\"n_features\"])]\n","  seq_len = tf.shape(input)[1]\n","  mask = create_masks(input[0], pad_value = conf[\"padding_values\"][0]) # input 0 = E, input 1 = r\n","\n","  ###### Encoder ###########\n","  x = 0 # will be casted to sum first embedding\n","  \n","  # Embedding layers\n","  for i in conf[\"enc_emb\"]:\n","    size = conf[\"vocab_sizes\"][i]\n","    embedding = tf.keras.layers.Embedding(int(size), conf[\"d_model\"])\n","    x += embedding(input[i]) # TODO: Multiply? Remove x=0?\n","  \n","  # Dense layers\n","  for i in conf[\"enc_dense\"]:\n","    dense = tf.keras.layers.Dense(conf[\"d_model\"])\n","    dense_out = dense(input[i])\n","    dense_out = tf.expand_dims(dense_out, axis=1)\n","    x += dense_out\n","  \n","  # Pos encoding\n","  pos_encoding = positional_encoding(conf[\"window_size\"], conf[\"d_model\"])\n","  pos_encoding = pos_encoding[:, :seq_len, :]\n","  x += pos_encoding\n","\n","  for i in range(conf[\"enc_num_layers\"]):\n","      enc_layer = EncoderLayer(conf[\"d_model\"], conf[\"num_heads\"], conf[\"dff\"], conf[\"dropout_rate\"])\n","      x = enc_layer(x, mask)\n","      out = x\n","  \n","  ####### Decoder #######\n","  y = 0 # will be casted to sum first embedding\n","  \n","  # Embedding layers\n","  for i in conf[\"dec_emb\"]:\n","    size = conf[\"vocab_sizes\"][i]\n","    embedding = tf.keras.layers.Embedding(int(size), conf[\"d_model\"])\n","    y += embedding(input[i]) # TODO: Multiply? Remove x=0?\n","\n","  # Dense layers\n","  for i in conf[\"dec_dense\"]:\n","    dense = tf.keras.layers.Dense(conf[\"d_model\"])\n","    dense_out = dense(input[i])\n","    dense_out = tf.expand_dims(dense_out, axis=1)\n","    y += dense_out\n","\n","  # Pos encoding\n","  y += pos_encoding\n","\n","  for i in range(conf[\"dec_num_layers\"]):\n","    dec_layer = DecoderLayer(conf[\"d_model\"], conf[\"num_heads\"], conf[\"dff\"], conf[\"dropout_rate\"])\n","    y = dec_layer(y, x, mask)\n","    out = y\n","  \n","  output = tf.keras.layers.Dense(3)(out) # final layer\n","\n","  # Instantiate an end-to-end model\n","  model = tf.keras.Model(input, output)\n","  return model"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"kC-4R9udKMjx"},"source":["# COPY OF WORKING MODEL!!\n","# def create_model(conf): # Functional model without list comprehension\n","#   input = tf.keras.Input(shape=(conf[\"window_size\"], conf[\"n_features\"]), name = \"input\")\n","#   seq_len = tf.shape(input)[1]\n","#   mask = create_masks(input[:,:,0], pad_value = conf[\"padding_values\"][0]) # input 0 = E, input 1 = r\n","\n","#   ###### Encoder ###########\n","#   x = 0 # will be casted to sum first embedding\n","  \n","#   # Embedding layers\n","#   for i in conf[\"enc_emb\"]:\n","#     size = conf[\"vocab_sizes\"][i]\n","#     embedding = tf.keras.layers.Embedding(int(size), conf[\"d_model\"])\n","#     x += embedding(input[:,:,i]) # TODO: Multiply? Remove x=0?\n","  \n","#   # Dense layers\n","#   for i in conf[\"enc_dense\"]:\n","#     dense = tf.keras.layers.Dense(conf[\"d_model\"])\n","#     dense_out = dense(input[:,:,i])\n","#     dense_out = tf.expand_dims(dense_out, axis=1)\n","#     x += dense_out\n","  \n","#   # Pos encoding\n","#   pos_encoding = positional_encoding(conf[\"window_size\"], conf[\"d_model\"])\n","#   pos_encoding = pos_encoding[:, :seq_len, :]\n","#   x += pos_encoding\n","\n","#   for i in range(conf[\"enc_num_layers\"]):\n","#       enc_layer = EncoderLayer(conf[\"d_model\"], conf[\"num_heads\"], conf[\"dff\"], conf[\"dropout_rate\"])\n","#       x = enc_layer(x, mask)\n","#       out = x\n","  \n","#   ####### Decoder #######\n","#   y = 0 # will be casted to sum first embedding\n","  \n","#   # Embedding layers\n","#   for i in conf[\"dec_emb\"]:\n","#     size = conf[\"vocab_sizes\"][i]\n","#     embedding = tf.keras.layers.Embedding(int(size), conf[\"d_model\"])\n","#     y += embedding(input[:,:,i]) # TODO: Multiply? Remove x=0?\n","\n","#   # Dense layers\n","#   for i in conf[\"dec_dense\"]:\n","#     dense = tf.keras.layers.Dense(conf[\"d_model\"])\n","#     dense_out = dense(input[:,:,i])\n","#     dense_out = tf.expand_dims(dense_out, axis=1)\n","#     y += dense_out\n","\n","#   # Pos encoding\n","#   y += pos_encoding\n","\n","#   for i in range(conf[\"dec_num_layers\"]):\n","#     dec_layer = DecoderLayer(conf[\"d_model\"], conf[\"num_heads\"], conf[\"dff\"], conf[\"dropout_rate\"])\n","#     y = dec_layer(y, x, mask)\n","#     out = y\n","  \n","#   output = tf.keras.layers.Dense(3)(out) # final layer\n","\n","#   # Instantiate an end-to-end model\n","#   model = tf.keras.Model(input, output)\n","#   return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4sGu7jgfb7Nj"},"source":["# shapes = [(None, ), (None, )]\n","# def create_model():\n","#   inputs = [tf.keras.Input(shape=shapes[i], name=selections_final[i]) for i in range(len(selections_final))]\n","#   seq_len = tf.shape(inputs[0])[1]\n","#   mask = create_masks(inputs[0], pad_value = pad_mapping[\"E\"])\n","  \n","#   enc_embeddings = [tf.keras.layers.Embedding(size, d_model) for size in input_vocab_size]\n","#   x = [enc_embeddings[i](inputs[i]) for i in range(len(inputs))]\n","#   x = tf.keras.layers.Add()(x)\n","\n","#   pos_encoding = positional_encoding(THR_E, d_model)\n","#   pos_encoding = pos_encoding[:, :seq_len, :]\n","  \n","#   x = tf.keras.layers.Add()([x, pos_encoding])\n","#   enc_layers = [EncoderLayer(d_model, num_heads, dff, dropout_rate) for _ in range(num_layers)]\n","  \n","#   for i in range(num_layers):\n","#       x = enc_layers[i](x, mask, training=True)\n","\n","#   output = tf.keras.layers.Dense(3)(x) # final layer\n","  \n","#   # Instantiate an end-to-end model\n","#   model = tf.keras.Model(inputs, output)\n","  \n","#   return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IZDggWNnQTw1"},"source":["# x = [emb * tf.math.sqrt(tf.cast(self.d_model, tf.float32)) for emb in x] # Scale it or not???"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"l8oMf5TXOAst"},"source":["# if q_inputs:\n","#     embs = [self.embedding(inp) for inp in q_inputs] # New: incorporate for more than one embedding. Each (batch_size, input_seq_len, d_model)\n","#     q = [emb * tf.math.sqrt(tf.cast(self.d_model, tf.float32)) for emb in embs] \n","#     q = tf.math.add_n(inputs=q)\n","#     q += self.pos_encoding[:, :seq_len, :]\n","#     q = self.dropout(q, training=training)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kj7hw06_lvli"},"source":["# # From https://machinetalk.org/2019/04/29/create-the-transformer-with-tensorflow-2-0/\n","# def positional_embedding(pos, model_size):\n","#     PE = np.zeros((1, model_size))\n","#     for i in range(model_size):\n","#         if i % 2 == 0:\n","#             PE[:, i] = np.sin(pos / 10000 ** (i / model_size))\n","#         else:\n","#             PE[:, i] = np.cos(pos / 10000 ** ((i - 1) / model_size))\n","#     return PE"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ljymFzOOcWIP"},"source":["# # https://stackoverflow.com/questions/63072898/tensorflow-2-how-to-use-stack-of-dense-layers-in-keras-functional-api\n","# def get_layers(inp, layer, n_layers, mask, training):\n","#     for i in range(n_layers):\n","#         x = layer(inp, mask, training)\n","#         inp = x\n","#     return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gMT-_tcQKmpA"},"source":["    # task_mask =  create_task_mask(inputs[:,:,-1]) \n","    # task_mask = tf.py_function(func=create_task_mask, inp=[inputs[:,:,-1]], Tout=tf.float32)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DvZyoTTANjyw"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OEdfvBYXNj5A"},"source":["# Numpy create task mask"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GLxurSWSNj5B"},"source":["# def create_task_mask(task_input):\n","#   seq = tf.cast(tf.math.equal(task_input, 1), tf.float32)\n","#   mask = np.apply_along_axis(get_task_one_mask, 1, seq)\n","#   mask = tf.expand_dims(mask, 1) # make compatible with padding mask\n","#   return mask"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yIeB39guNj5B"},"source":["# def get_task_one_mask(A): # https://stackoverflow.com/questions/65045232/set-values-in-row-to-zero-before-index-value-of-row-numpy-or-tensorflow\n","#   N = A.shape[0]\n","#   column = (A > 0).reshape((N, 1))\n","#   mask = np.ones((N, N), dtype=np.bool)\n","#   mask = np.where(column, False, np.tril(mask, -1))\n","#   mask = np.cumsum(mask, axis=0)\n","#   B = np.where(mask, 0, np.tile(A, (N, 1)))\n","#   return B"],"execution_count":null,"outputs":[]}]}