{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"riiid-preprocessing.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNyGmgjhmx6GKMVB5V3g0Rm"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"MxypEzmP1FSa"},"source":["# Logging \n","# set up logging to file - see previous section for more details\n","def create_logging(OUTPUT_FOLDER):\n","    logging.basicConfig(level=logging.DEBUG,\n","                        format='%(asctime)s %(name)-12s %(levelname)-8s %(message)s',\n","                        datefmt='%d-%m %H:%M:%S',\n","                        filename= os.path.join(OUTPUT_FOLDER, 'logger.log'),\n","                        filemode='w')\n","\n","    # define a Handler which writes INFO messages or higher to the sys.stderr or sys.stdout\n","    console = logging.StreamHandler()\n","    console.setLevel(logging.INFO)\n","    # set a format which is simpler for console use\n","    formatter = logging.Formatter(fmt='%(asctime)s %(name)-12s %(levelname)-8s %(message)s',\n","                                  datefmt='%d-%m %H:%M')\n","    # tell the handler to use this format\n","    console.setFormatter(formatter)\n","    # add the handler to the root logger\n","    logging.getLogger().addHandler(console)\n","    return logging"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gTVGdnnpuR8L"},"source":["def read_df_print(path, format = \"feather\"): # TODO: can change to other formats too\n","  if format == \"feather\":\n","    df = pd.read_feather(path)\n","  print(df.shape)\n","  display(df.head(3))\n","  return df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"O8VIbPEEm07s"},"source":["# Split users into multiple parts based on THR_E\n","def split_into_more_users(train, ascending=True):\n","  train[\"user_cumcount\"] = train.groupby(\"user_id\").cumcount(ascending)\n","  train[\"cumcount_thr\"] = (train[\"user_cumcount\"]/THR_E).astype(\"int\")\n","  train[\"new_user_id\"] = train.groupby([\"user_id\", \"cumcount_thr\"]).ngroup()\n","  del train[\"user_cumcount\"], train[\"cumcount_thr\"]\n","  return train"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gALkAThsoYb-"},"source":["# Gets a feature as a sequence in lists (with pd.Series)\n","def get_user_sequence(feature, groupby_id = \"new_user_id\", to_dict = False):\n","  # user_seq = train.loc[train[\"content_type_id\"]==0].groupby(groupby_id)[feature].apply(list)\n","  user_seq = train.groupby(groupby_id)[feature].apply(list)\n","  if to_dict:\n","    user_seq = user_seq.to_dict()\n","  return user_seq"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b6KptCZWo9fy"},"source":["def standardization(feature_df, feature_train): # feature_df can be of train again or test\n","    standard = (feature_df - np.nanmean(feature_train))/np.nanstd(feature_train)\n","    return standard"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wu4WoWwHo-gz"},"source":["# Exercises\n","def return_E(groupby_id=\"new_user_id\"):\n","  E_lists = get_user_sequence(\"content_id\", groupby_id)\n","  return E_lists"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lYkfkgm2qPGf"},"source":["# Results r\n","def return_r(add_start_token, groupby_id=\"new_user_id\"):\n","  # Add results\n","  r_lists = get_user_sequence(\"answered_correctly\", groupby_id) # All results (r)\n","\n","  # Add start token to r_list\n","  if add_start_token:\n","    r_lists = r_lists.apply(lambda x: [N_response+1] + x)\n","  return r_lists"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aRrVeiO8rfTn"},"source":["# Task\n","def return_task_binary(groupby_id=\"new_user_id\"):\n","  # Add whether exercise was part of a task container\n","    train[\"task_binary\"] = train[[\"user_id\", \"task_container_id\"]].duplicated(keep=False).astype(\"int8\") # with user_id!\n","    task_lists = get_user_sequence(\"task_binary\", groupby_id)\n","    del train[\"task_binary\"]\n","    return task_lists"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9XiDEUgQCTdN"},"source":["# # Bundle binary value => 0/1\n","# def return_bundle_binary(groupby_id=\"new_user_id\"):\n","#     # Add whether exercise was part of the same task container i.e. bundle id\n","#     train[\"bundle_binary\"] = train[[\"user_id\", \"task_container_id\"]].duplicated(keep=False).astype(\"int8\")\n","#     task_lists = get_user_sequence(\"bundle_binary\", groupby_id)\n","#     del train[\"bundle_binary\"]\n","#     return bundle_lists"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OJcJmNJhylxo"},"source":["# Part\n","def return_p(groupby_id=\"new_user_id\"):\n","    part_dict = dict(zip(questions.question_id, questions.part))\n","    train[\"part\"]= train[\"content_id\"].map(part_dict).fillna(0).astype(\"int8\")\n","    p_lists = get_user_sequence(\"part\", groupby_id) # All parts (p) of the exercises\n","    del train[\"part\"]\n","    return p_lists"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5b7TLFmtxO-T"},"source":["# Elapsed time\n"," # TODOs: 1) make categorical 2) change error first task container (mentioned in discsussions)\n","def return_et(groupby_id=\"new_user_id\"):\n","    train[\"et\"] = train[\"prior_question_elapsed_time\"].fillna(0)/300000\n","    train[\"et\"]= train[\"et\"].astype(\"float32\")\n","    et_lists = get_user_sequence(\"et\", groupby_id) # Elapsed  times\n","    return et_lists"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dtA8VLIFMCMf"},"source":["# Elapsed time\n"," # TODOs: 1) make categorical 2) change error first task container (mentioned in discsussions)\n","def return_et_std(groupby_id=\"new_user_id\"):\n","    quantile_transformer = []\n","    train[\"et\"] = train[\"prior_question_elapsed_time\"].fillna(0).astype(\"float32\")\n","\n","    # TODO: only fit_transform on train, transform on val?\n","    quantile_transformer = preprocessing.QuantileTransformer(random_state=0)\n","    train_len = int(train[\"new_user_id\"].nunique()*0.9)\n","    \n","    qt_transform_train = quantile_transformer.fit_transform(train.loc[train[\"new_user_id\"]<train_len, \"et\"].values.reshape(-1, 1))\n","    qt_transform_val = quantile_transformer.transform(train.loc[train[\"new_user_id\"]>=train_len, \"et\"].values.reshape(-1, 1))\n","    \n","    train.loc[train[\"new_user_id\"]<train_len, \"et_std\"] = qt_transform_train\n","    train.loc[train[\"new_user_id\"]>=train_len, \"et_std\"] = qt_transform_val\n","\n","    et_lists = get_user_sequence(\"et_std\", groupby_id) # Elapsed  times\n","    del train[\"et\"], train[\"et_std\"]\n","    return et_lists, quantile_transformer"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YIW0DVQQJg8c"},"source":["# # Elapsed time\n","#  # TODOs: 1) make categorical 2) change error first task container (mentioned in discsussions)\n","# def return_et_std(groupby_id=\"new_user_id\"):\n","#     quantile_transformer = []\n","#     train[\"et\"] = train[\"prior_question_elapsed_time\"].fillna(0)/300000\n","#     train[\"et\"]= train[\"et\"].astype(\"float32\")\n","#     et_lists = get_user_sequence(\"et\", groupby_id) # Elapsed  times\n","    \n","#     # Quantile transformer. TODO: only fit_transform on train, transform on val?\n","#     quantile_transformer = preprocessing.QuantileTransformer(random_state=0)\n","#     train[\"et_std\"] = quantile_transformer.fit_transform(train[\"et\"].values.reshape(-1, 1))\n","#     et_lists = get_user_sequence(\"et_std\", groupby_id) # Elapsed  times\n","#     del train[\"et\"], train[\"et_std\"]\n","#     return et_lists, quantile_transformer"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8gNuGILNslAs"},"source":["# Lag time\n","def return_lt(groupby_id=\"new_user_id\"):\n","    train[\"lt\"] = train[\"timestamp\"].diff()\n","\n","    train[\"task_cumcount\"] = train.groupby(\"user_id\")[\"task_container_id\"].cumcount() # use user_id or new_user_id???\n","    train.loc[train[\"prior_question_elapsed_time\"].isnull(), \"lt\"] = np.nan # lectures/first user\n","    train.loc[train[\"task_cumcount\"]==0, \"lt\"] = np.nan # first bundle\n","    del train[\"task_cumcount\"]\n","\n","    train[\"lt\"] = train[\"lt\"].fillna(0)/1000 #  fill NA as zero seconds + convert to seconds\n","    assert((train[\"lt\"]<0).sum()==0) # There should be no negative time differences\n","\n","    # Transformation of lt. Currently implemented: QuantielTransformer\n","    # train[\"std_lt\"] = train[\"lt\"]**(1/4)\n","    quantile_transformer = preprocessing.QuantileTransformer(random_state=0)\n","    train[\"lt_std\"] = quantile_transformer.fit_transform(train[\"lt\"].values.reshape(-1, 1))\n","    train[\"lt_std\"] = train[\"lt_std\"].astype(\"float32\")\n","    lt_lists = get_user_sequence(\"lt_std\", groupby_id) \n","    \n","    # train[\"lt\"].isnull().sum() # +- sum of users + lectures\n","    del train[\"lt\"], train[\"lt_std\"]\n","    return lt_lists"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uzGDLTyFy5R1"},"source":["# Question tags\n","def keep_N_highest_tags(x, pad_value, max_tags=1): # TODO: not completed yet!!\n","    tags, count = np.array(x[\"tag\"]), np.array(x[\"tag_count\"])\n","    ind = count.argsort()[-max_tags:][::-1] # https://stackoverflow.com/questions/6910641/how-do-i-get-indices-of-n-maximum-values-in-a-numpy-array\n","    if len(tags)>0:\n","      N_highest_tags = tags[ind[0]]\n","    else:\n","      N_highest_tags = pad_value\n","    return N_highest_tags\n","\n","def return_N_highest_tags(groupby_id=\"new_user_id\"):\n","    empty_list = []\n","    questions[\"tag\"] = questions[\"tags\"].apply(lambda x: x.split(\" \") if pd.notnull(x) else empty_list)\n","    len_tags = questions[\"tag\"].apply(lambda x: len(x) if isinstance(x, list) else 0)\n","    tags_count = Counter(x for xs in questions[\"tag\"] for x in set(xs)) # https://stackoverflow.com/questions/19211018/using-counter-with-list-of-lists\n","    questions[\"tag_count\"] = questions[\"tag\"].apply(lambda x: [tags_count[i] for i in x])\n","\n","    # Apply functions to dataframe. TODO: not completed yet!!\n","    N_tags, max_tags = len(tags_count), 1\n","    questions[\"N_highest_tag\"] = questions.apply(keep_N_highest_tags, axis=1, args=(N_tags, max_tags))\n","\n","    # map to train\n","    N_highest_tag_dict = dict(zip(questions.question_id, questions.N_highest_tag))\n","    train[\"N_highest_tag\"]= train[\"content_id\"].map(N_highest_tag_dict).fillna(0).astype(\"int32\")\n","\n","    # get sequence\n","    tag_lists = get_user_sequence(\"N_highest_tag\", groupby_id)\n","    del train[\"N_highest_tag\"]\n","    return tag_lists"],"execution_count":null,"outputs":[]}]}