{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "demo-riiid-train-lectures.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1alPO9cYR8zX",
        "outputId": "955131bf-97e2-405b-83ac-4ba817009c10"
      },
      "source": [
        "# Install latest version sklearn (0.23)\n",
        "!pip install -U scikit-learn"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting scikit-learn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/30/aa/db462d385c56905b731403885454188683f63c86ea68900f6f7e7558b5fa/scikit_learn-0.24.0-cp36-cp36m-manylinux2010_x86_64.whl (22.2MB)\n",
            "\u001b[K     |████████████████████████████████| 22.2MB 1.4MB/s \n",
            "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f7/12/ec3f2e203afa394a149911729357aa48affc59c20e2c1c8297a60f33f133/threadpoolctl-2.1.0-py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.19.4)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.0.0)\n",
            "Installing collected packages: threadpoolctl, scikit-learn\n",
            "  Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "Successfully installed scikit-learn-0.24.0 threadpoolctl-2.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "id": "fl0JGKUzPcs0"
      },
      "source": [
        "# Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import shutil\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import gc\n",
        "import sys\n",
        "import random\n",
        "import pickle\n",
        "\n",
        "from scipy.stats import iqr\n",
        "from sklearn import preprocessing\n",
        "\n",
        "import datetime\n",
        "import logging\n",
        "\n",
        "# Deep learning\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "import tensorflow.keras.layers as L\n",
        "import tensorflow.keras.models as M\n",
        "\n",
        "from IPython.display import display\n",
        "# Set pandas options\n",
        "pd.options.display.max_rows = 2000"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "id": "TyCCeSTaw6_s"
      },
      "source": [
        "# Global variables \n",
        "THR_E = 100 # interaction threshold of exercises (E) for user\n",
        "BATCH_SIZE = 128\n",
        "TRAIN_FRACTION = 0.95\n",
        "\n",
        "AUTO = tf.data.experimental.AUTOTUNE\n",
        "EPOCHS = 180\n",
        "N_SELECT_PER_EPOCH = 100000 # Random select N samples for each epoch from train/val set\n",
        "VAL_EVERY_N_EPOCHS = 5\n",
        "PRINT_EVERY_N_BATCHES = 50 \n",
        "zero_task_etc = True\n",
        "\n",
        "SAVE_DICTS = True"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4M-mWBNVcaQ7"
      },
      "source": [
        "# SEED THE EXPERIMENTS\r\n",
        "np.random.seed(18)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XR5Io93kU0RK"
      },
      "source": [
        "# Experiment date and number!\r\n",
        "date = datetime.datetime.today().strftime(\"%d-%b\")\r\n",
        "experiment = \"-riiid-1\"\r\n",
        "OUTPUT_FOLDER = date + experiment \r\n",
        "os.makedirs(OUTPUT_FOLDER, exist_ok=True)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPFvJd9PsbR6"
      },
      "source": [
        "# Selected features\n",
        "selections = [\"E\", \"r\", \"etc\", \"ltg\", \"at\", \"ra\"]\n",
        "assert((selections[0] == \"E\") & (selections[1]==\"r\"))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wiHtMtRWAvE8"
      },
      "source": [
        "# Choose model setting\n",
        "ENC_EMB, ENC_DENSE = [0, 4], [] \n",
        "DEC_EMB, DEC_DENSE = [1, 2, 3, 5], []"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRrlNtCi8QxP",
        "outputId": "7660adfb-3e17-4aff-f2e3-9593a495cc86"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "FOLDER_FEATHER = \"/content/drive/My Drive/kaggle-riiid/feather-files\"\n",
        "PREPROCESS_FILE = \"/content/drive/My\\ Drive/Colab\\ Notebooks/demo-riiid-preprocessing.ipynb\"\n",
        "MODEL_FILE = \"/content/drive/My\\ Drive/Colab\\ Notebooks/demo-riiid-transformer.ipynb\" # https://stackoverflow.com/questions/57464810/how-to-run-a-jupyter-notebook-with-space-in-relative-path-from-another-notebook\n",
        "# MODEL_FILE = \"/content/drive/My\\ Drive/Colab\\ Notebooks/riiid-functional-transformer.ipynb\" # https://stackoverflow.com/questions/57464810/how-to-run-a-jupyter-notebook-with-space-in-relative-path-from-another-notebook\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2pN4ICUn9tK"
      },
      "source": [
        "# Add functions to preprocess data\n",
        "%run $PREPROCESS_FILE"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eoay_2ef1W7r"
      },
      "source": [
        "# Create logger\n",
        "logging = create_logging(OUTPUT_FOLDER)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "id": "4aC9MfDSbI0c",
        "outputId": "7954bbe5-c526-458a-804e-4be47d01cc7e"
      },
      "source": [
        "%%time\n",
        "# Read all dataframes from feather files and print out\n",
        "train = read_df_print(os.path.join(FOLDER_FEATHER, \"train.feather\")) \n",
        "questions = read_df_print(os.path.join(FOLDER_FEATHER, \"questions.feather\"))\n",
        "lectures = read_df_print(os.path.join(FOLDER_FEATHER, \"lectures.feather\"))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(101230332, 10)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>row_id</th>\n",
              "      <th>timestamp</th>\n",
              "      <th>user_id</th>\n",
              "      <th>content_id</th>\n",
              "      <th>content_type_id</th>\n",
              "      <th>task_container_id</th>\n",
              "      <th>user_answer</th>\n",
              "      <th>answered_correctly</th>\n",
              "      <th>prior_question_elapsed_time</th>\n",
              "      <th>prior_question_had_explanation</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>115</td>\n",
              "      <td>5692</td>\n",
              "      <td>False</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>56943</td>\n",
              "      <td>115</td>\n",
              "      <td>5716</td>\n",
              "      <td>False</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>37000.0</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>118363</td>\n",
              "      <td>115</td>\n",
              "      <td>128</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>55000.0</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   row_id  ...  prior_question_had_explanation\n",
              "0       0  ...                            None\n",
              "1       1  ...                           False\n",
              "2       2  ...                           False\n",
              "\n",
              "[3 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "(13523, 5)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question_id</th>\n",
              "      <th>bundle_id</th>\n",
              "      <th>correct_answer</th>\n",
              "      <th>part</th>\n",
              "      <th>tags</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>51 131 162 38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>131 36 81</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>131 101 162 92</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   question_id  bundle_id  correct_answer  part            tags\n",
              "0            0          0               0     1   51 131 162 38\n",
              "1            1          1               1     1       131 36 81\n",
              "2            2          2               0     1  131 101 162 92"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "(418, 4)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>lecture_id</th>\n",
              "      <th>tag</th>\n",
              "      <th>part</th>\n",
              "      <th>type_of</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>89</td>\n",
              "      <td>24584</td>\n",
              "      <td>5</td>\n",
              "      <td>concept</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>100</td>\n",
              "      <td>22243</td>\n",
              "      <td>1</td>\n",
              "      <td>concept</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>185</td>\n",
              "      <td>7035</td>\n",
              "      <td>6</td>\n",
              "      <td>concept</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   lecture_id    tag  part  type_of\n",
              "0          89  24584     5  concept\n",
              "1         100  22243     1  concept\n",
              "2         185   7035     6  concept"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1.62 s, sys: 1.92 s, total: 3.54 s\n",
            "Wall time: 29.3 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PrxymFQ6jkrL",
        "outputId": "15dc6ba5-42c3-4869-fc80-535286e1e4e5"
      },
      "source": [
        "train[\"content_type_id\"].sum()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1959032"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKCkG5SuiEnQ"
      },
      "source": [
        "# Preprocess for lectures\n",
        "if (\"v\" in selections) or (\"l\" in selections) or (\"vc\" in selections):\n",
        "  \n",
        "  lectures[\"lecture_id\"] = pd.Categorical(lectures[\"lecture_id\"])\n",
        "  lectures[\"lec_id\"] = lectures[\"lecture_id\"].cat.codes\n",
        "  lec_map = dict(zip(lectures[\"lecture_id\"], lectures[\"lec_id\"]))\n",
        "\n",
        "  train.loc[train[\"content_type_id\"]==1, \"content_id\"] = train.loc[train[\"content_type_id\"]==1, \"content_id\"].map(lec_map)\n",
        "  train[\"v\"] = train[\"content_type_id\"].shift(1).fillna(0).astype(np.int8)\n",
        "  train[\"l\"] = train[\"content_id\"].shift(1).fillna(418).astype(np.int32)\n",
        "  train.loc[train[\"v\"]==0, \"l\"] = 418\n",
        "  train[\"vc\"] = train.groupby(\"user_id\")[\"v\"].cumsum()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifHkb8aPPhma",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88fa4a86-88c7-483c-d2a3-414d5d2947fc"
      },
      "source": [
        "# We don't need the lectures now!\n",
        "train = train.loc[train[\"content_type_id\"]==0]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "29-12 18:24 numexpr.utils INFO     NumExpr defaulting to 4 threads.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XqeE-GuynMui"
      },
      "source": [
        "# Save space by deleting some columns\n",
        "del train[\"user_answer\"]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "id": "yjODj3l-Eas9"
      },
      "source": [
        "N_questions, N_parts = questions[\"question_id\"].nunique(), questions[\"part\"].nunique()\n",
        "N_response, N_task, N_lag, N_et, N_groups, N_attempt, N_avg, N_ltg, N_l = 2, 2, 1440, 301, 10, 8, 101, 400, 418 # N_avg = 100 for regular?"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bp-gBbzHlND"
      },
      "source": [
        "# Seq len of users [sorted same as df]. USED FOR TRAINING\n",
        "seq_len_dict = dict(train[\"user_id\"].value_counts())\n",
        "users = train[\"user_id\"].unique()\n",
        "seq_lens = [seq_len_dict[user] for user in users]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kF2zoN0jRqzV"
      },
      "source": [
        "# Train/valid split\r\n",
        "train_len = int(len(users)*TRAIN_FRACTION)\r\n",
        "val_len = len(users) - train_len\r\n",
        "train_users, val_users = users[:train_len], users[train_len:]\r\n",
        "\r\n",
        "# PROBS of sequence\r\n",
        "seq_len_train, seq_len_val = seq_lens[:train_len], seq_lens[train_len:]\r\n",
        "PROBS_TRAIN = seq_len_train/np.sum(seq_len_train)\r\n",
        "PROBS_VAL = seq_len_val/np.sum(seq_len_val)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5pBSw-YFjgZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce3d9ff1-fb88-4fa7-aea1-b4f3a2ae1fd2"
      },
      "source": [
        "len(train_users), len(val_users)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(373973, 19683)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhH7a4bGQDjZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b40e113d-3a4d-4a40-9f99-ec0c8c1a720f"
      },
      "source": [
        "train[\"prior_question_had_explanation\"].isnull().sum() # Less than users - why?"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "392506"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWvX2zk-AIBg"
      },
      "source": [
        "## Add all inputs/outputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8P_CP-lACNhq"
      },
      "source": [
        "p_lists, lt_lists, et_lists, tag_lists, task_lists, quantile_transformer_et, lt_cat, quantile_transformer_lt, et_cat, ltc, etc, ex, etg, ra, ca, at, r_dup, ltg, ltg_bins, v_lists, l_lists  = [], [], [], [], [], [], [], [], [], [],[], [], [], [], [], [], [], [], [], [], []"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgJckHd9R47p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f02bb532-66c9-4aae-f74e-c6b8b4206617"
      },
      "source": [
        "if \"ra\" in selections:\r\n",
        "    %time ra = return_r_avg(add_start_token=True, N_avg = N_avg)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 33.2 s, sys: 606 ms, total: 33.8 s\n",
            "Wall time: 33 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTBlz8gcKwIq",
        "outputId": "3cac3027-d2d7-40d9-8b9e-b61deb8487a3"
      },
      "source": [
        "# Add exercises\n",
        "%time E_lists = return_E()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 22.1 s, sys: 1.26 s, total: 23.3 s\n",
            "Wall time: 23.3 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9NhhuL1-QDUT",
        "outputId": "55340b75-719f-43fe-d5fe-4662a44247a3"
      },
      "source": [
        "# Add results, with or without start token\n",
        "%time r_lists = return_r(add_start_token=True)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 26.8 s, sys: 253 ms, total: 27 s\n",
            "Wall time: 27 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUDcPATyYwkN"
      },
      "source": [
        "if \"l\" in selections:\r\n",
        "  %time l_lists = return_l()\r\n",
        "  del train[\"l\"], train[\"v\"], train[\"vc\"]\r\n",
        "if \"v\" in selections:\r\n",
        "    %time v_lists = return_v()\r\n",
        "    del train[\"l\"], train[\"v\"], train[\"vc\"]"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQw0mNR3QJy3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0c3366b-988d-4534-e0f5-623ccbb17d1c"
      },
      "source": [
        "if \"at\" in selections:\r\n",
        "    %time at = return_attempt()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 56.8 s, sys: 5.66 s, total: 1min 2s\n",
            "Wall time: 1min 2s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqqgBFwxvMI6",
        "outputId": "15513bf8-7226-4f4a-803e-4bc28ed73eff"
      },
      "source": [
        "# Lag time grouped\r\n",
        "if \"ltg\" in selections:\r\n",
        "    %time ltg, ltg_bins, N_ltg = return_ltg(N_ltg)    \r\n",
        "print(N_ltg)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 39.7 s, sys: 255 ms, total: 39.9 s\n",
            "Wall time: 39.3 s\n",
            "173\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hp6pkC62ZRw"
      },
      "source": [
        "if \"r_dup\" in selections:\r\n",
        "   %time r_dup = r_lists.apply(lambda x: x[:-1])"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uA-ropTQLh81"
      },
      "source": [
        "if \"ca\" in selections:\r\n",
        "    %time ca = return_ca()"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCZ0-bcyNAxi"
      },
      "source": [
        "if \"ex\" in selections:\r\n",
        "    %time ex = return_ex()"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrREQHH1RR84"
      },
      "source": [
        "bins = list(range(30)) + [32, 35, 40, 48, 60, 80, 120, 299, 300]\r\n",
        "if \"etg\" in selections:\r\n",
        "    %time etg = return_etg(bins)\r\n",
        "    # %time etg = return_etg(N_groups)\r\n"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7OIGHkzpVYN"
      },
      "source": [
        "if \"ltc\" in selections:\n",
        "    %time ltc = return_ltc()"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFCxAcxx_6Op"
      },
      "source": [
        "if \"lt\" in selections:\n",
        "    %time lt_lists, quantile_transformer_lt = return_lt()"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4lfwLZ1UHFE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "014ac44b-bdf8-43c6-df72-e50993eb4191"
      },
      "source": [
        "if \"etc\" in selections:\r\n",
        "    %time etc = return_etc(zero_task=zero_task_etc)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 34 s, sys: 604 ms, total: 34.6 s\n",
            "Wall time: 34.4 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRFLPPQ4_6Om"
      },
      "source": [
        "if \"p\" in selections:\n",
        "    %time p_lists = return_p()"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEUWCAAXNsl3"
      },
      "source": [
        "if \"et\" in selections:\r\n",
        "    %time et_lists, quantile_transformer_et = return_et()"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2pE03BaNqid"
      },
      "source": [
        "if \"tag\" in selections:\n",
        "    %time tag_lists = return_N_highest_tags() # TODO: specify N tags"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gF9IXoPo8VZW"
      },
      "source": [
        "if \"task\" in selections:\n",
        "    %time task_lists = return_task_binary()"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUSTsXKtEX4o"
      },
      "source": [
        "feature_mapping = {\"E\": E_lists, \"r\": r_lists, \"p\": p_lists, \"et\": et_lists, \"lt\": lt_lists, \"tag\": tag_lists, \"task\": task_lists, \"et_std\": et_lists, \"ltc\": ltc, \"etc\": etc, \"ex\": ex, \"etg\": etg, \"ra\": ra, \"at\": at, \"ca\": ca, \"r_dup\": r_dup, \"v\": v_lists, \"ltg\": ltg, \"l\": l_lists}\n",
        "type_mapping = {\"E\": tf.int32, \"r\": tf.int32, \"p\": tf.int32, \"et\": tf.float32, \"et_std\": tf.float32, \"lt\": tf.float32, \"tag\": tf.float32, \"task\": tf.float32, \"ltc\":tf.int32, \"etc\": tf.int32, \"ex\": tf.int32, \"etg\": tf.int32, \"ra\": tf.int32, \"at\": tf.int32, \"ca\": tf.int32, \"r_dup\": tf.int32, \"v\": tf.int32, \"ltg\": tf.int32, \"l\": tf.int32}\n",
        "pad_mapping = {\"E\": N_questions, \"r\": N_response, \"p\": 0, \"et\": 0.5, \"et_std\": 0.0, \"lt\": 0.5, \"tag\": 2.0, \"task\": float(N_task), \"ltc\": N_lag+1, \"etc\": N_et+1, \"ex\": 2, \"etg\": len(bins)+1, \"ra\": N_avg+1, \"at\": N_attempt, \"ca\": N_avg+1, \"r_dup\": N_response, \"v\": N_response, \"ltg\": N_ltg+1, \"l\": N_l+1}\n",
        "vocab_mapping = {\"E\": N_questions+1, \"r\": N_response+2, \"et\":None, \"p\": N_parts+1, \"lt\": None, \"tag\": 3.0, \"task\": float(N_task+1), \"et_std\": float(300), \"ltc\": N_lag+2, \"etc\": N_et+2, \"ex\": 4, \"etg\": len(bins)+2, \"ra\": N_avg+2, \"at\": N_attempt+1, \"ca\": N_avg+2, \"r_dup\": N_response+2, \"v\": N_response+1, \"ltg\": N_ltg+2, \"l\": N_l+2}\n",
        "pad_shapes = {\"E\": [THR_E], \"r\": [THR_E+1], \"p\": [THR_E], \"et\": [THR_E], \"et_std\": [THR_E],  \"lt\": [THR_E], \"tag\": [THR_E], \"task\": [THR_E], \"ltc\": [THR_E], \"etc\": [THR_E], \"ex\": [THR_E], \"etg\": [THR_E], \"ra\": [THR_E], \"at\": [THR_E], \"ca\": [THR_E], \"r_dup\": [THR_E], \"v\": [THR_E], \"ltg\": [THR_E], \"l\": [THR_E]}"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_g8Yp5YRrtXw"
      },
      "source": [
        "def save_as_dict(df, filename):\n",
        "    seq_dict = df.to_dict()\n",
        "    with open(os.path.join(OUTPUT_FOLDER_DICT, filename), 'wb') as handle:\n",
        "      pickle.dump(seq_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poOMvtlCq1Pl"
      },
      "source": [
        "if SAVE_DICTS:\n",
        "  OUTPUT_FOLDER_DICT = f\"seq-dict-{THR_E}\"\n",
        "  os.makedirs(OUTPUT_FOLDER_DICT, exist_ok=True)\n",
        "\n",
        "  for feature in selections:\n",
        "      feat_df = feature_mapping[feature]\n",
        "      feat_df = feat_df.apply(lambda x: x[-THR_E:]) # Take last THR_E in history \n",
        "      filename = f\"{feature}.pickle\"\n",
        "      save_as_dict(feat_df, filename)\n",
        "  shutil.move(OUTPUT_FOLDER_DICT, OUTPUT_FOLDER)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipaNyd2L4E3f"
      },
      "source": [
        "# # Add start token!\r\n",
        "# # Answers\r\n",
        "# r_lists = r_lists.apply(lambda x: [3] + x) # Start token = 3\r\n",
        "# feature_mapping[\"r\"] = r_lists"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aQlsiawcO-8"
      },
      "source": [
        "# Running answers\r\n",
        "if \"ra\" in selections:\r\n",
        "    ra = ra.apply(lambda x: x[:-1]) # For training => we don't have last value of average\r\n",
        "    feature_mapping[\"ra\"] = ra"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtGXR1IZvyKP"
      },
      "source": [
        "vocab_sizes = [vocab_mapping[select] for select in selections]\n",
        "feature_lists = [feature_mapping[select] for select in selections]"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ai9vHiP4I1hI"
      },
      "source": [
        "PADDING_VALUES = tuple((pad_mapping[select] for select in selections))\n",
        "OUTPUT_TYPES = tuple((type_mapping[select] for select in selections))\n",
        "PADDED_SHAPES = tuple((pad_shapes[select] for select in selections))"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sO-UUR-V21KT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a96cc3be-9aa6-45b7-eee7-8945eac1a7de"
      },
      "source": [
        "feature_lists"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[user_id\n",
              " 115           [5692, 5716, 128, 7860, 7922, 156, 51, 50, 789...\n",
              " 124           [7900, 7876, 175, 1278, 2064, 2063, 2065, 3364...\n",
              " 2746          [5273, 758, 5976, 236, 404, 382, 405, 873, 531...\n",
              " 5382          [5000, 3944, 217, 5844, 5965, 4990, 5235, 6050...\n",
              " 8623          [3915, 4750, 6456, 3968, 6104, 5738, 6435, 549...\n",
              "                                     ...                        \n",
              " 2147470770    [7900, 7876, 175, 1278, 2064, 2065, 2063, 3363...\n",
              " 2147470777    [7900, 7876, 175, 1278, 2065, 2064, 2063, 3365...\n",
              " 2147481750    [4137, 1270, 9261, 8201, 367, 378, 214, 6071, ...\n",
              " 2147482216    [3748, 4765, 5474, 9261, 4665, 5987, 6666, 561...\n",
              " 2147482888    [6147, 4792, 5738, 6102, 4748, 7956, 6435, 928...\n",
              " Name: content_id, Length: 393656, dtype: object, user_id\n",
              " 115           [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, ...\n",
              " 124           [3, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, ...\n",
              " 2746          [3, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, ...\n",
              " 5382          [3, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, ...\n",
              " 8623          [3, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, ...\n",
              "                                     ...                        \n",
              " 2147470770    [3, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, ...\n",
              " 2147470777    [3, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, ...\n",
              " 2147481750    [3, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, ...\n",
              " 2147482216    [3, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, ...\n",
              " 2147482888    [3, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, ...\n",
              " Name: answered_correctly, Length: 393656, dtype: object, user_id\n",
              " 115           [301, 37, 55, 19, 11, 5, 17, 17, 16, 16, 17, 2...\n",
              " 124           [301, 26, 29, 26, 18, 0, 0, 33, 0, 0, 21, 0, 0...\n",
              " 2746          [301, 28, 17, 24, 20, 16, 16, 19, 18, 18, 20, ...\n",
              " 5382          [301, 24, 35, 88, 18, 12, 5, 92, 70, 14, 79, 6...\n",
              " 8623          [301, 16, 33, 30, 40, 35, 30, 29, 15, 19, 14, ...\n",
              "                                     ...                        \n",
              " 2147470770    [301, 21, 22, 26, 16, 0, 0, 25, 0, 0, 24, 0, 0...\n",
              " 2147470777    [301, 23, 23, 23, 17, 0, 0, 26, 0, 0, 18, 0, 0...\n",
              " 2147481750    [301, 19, 18, 8, 21, 15, 15, 18, 17, 14, 22, 2...\n",
              " 2147482216    [301, 38, 27, 17, 9, 12, 28, 22, 25, 15, 19, 1...\n",
              " 2147482888    [301, 15, 18, 21, 21, 16, 20, 41, 41, 30, 18, ...\n",
              " Name: etc, Length: 393656, dtype: object, user_id\n",
              " 115           [173, 49, 53, 5, 1, 11, 11, 10, 10, 11, 16, 17...\n",
              " 124           [173, 25, 21, 14, 92, 0, 0, 61, 0, 0, 64, 0, 0...\n",
              " 2746          [173, 14, 19, 15, 12, 12, 15, 88, 41, 37, 21, ...\n",
              " 5382          [173, 32, 83, 14, 7, 1, 86, 66, 10, 76, 64, 15...\n",
              " 8623          [173, 31, 26, 36, 31, 26, 24, 11, 14, 10, 33, ...\n",
              "                                     ...                        \n",
              " 2147470770    [173, 17, 20, 10, 70, 0, 0, 69, 0, 0, 45, 0, 0...\n",
              " 2147470777    [173, 19, 157, 12, 99, 0, 0, 94, 0, 0, 57, 0, ...\n",
              " 2147481750    [173, 15, 3, 16, 10, 10, 16, 12, 9, 17, 67, 14...\n",
              " 2147482216    [173, 23, 12, 4, 7, 22, 17, 20, 129, 51, 39, 4...\n",
              " 2147482888    [173, 13, 15, 16, 11, 15, 35, 168, 30, 48, 139...\n",
              " Name: ltg, Length: 393656, dtype: object, user_id\n",
              " 115           [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              " 124           [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              " 2746          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ...\n",
              " 5382          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              " 8623          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              "                                     ...                        \n",
              " 2147470770    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              " 2147470777    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              " 2147481750    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              " 2147482216    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              " 2147482888    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              " Name: attempt, Length: 393656, dtype: object, user_id\n",
              " 115           [101, 100, 100, 100, 100, 100, 100, 100, 100, ...\n",
              " 124           [101, 100, 50, 67, 50, 40, 33, 43, 38, 33, 30,...\n",
              " 2746          [101, 0, 0, 0, 25, 20, 33, 43, 50, 56, 50, 55,...\n",
              " 5382          [101, 100, 50, 67, 50, 60, 67, 71, 75, 67, 60,...\n",
              " 8623          [101, 100, 100, 100, 100, 100, 100, 100, 88, 7...\n",
              "                                     ...                        \n",
              " 2147470770    [101, 100, 100, 100, 100, 100, 100, 86, 75, 67...\n",
              " 2147470777    [101, 0, 50, 33, 50, 60, 50, 43, 50, 56, 50, 4...\n",
              " 2147481750    [101, 0, 0, 33, 50, 60, 67, 71, 75, 78, 70, 73...\n",
              " 2147482216    [101, 0, 0, 0, 25, 40, 33, 29, 25, 22, 30, 27,...\n",
              " 2147482888    [101, 100, 100, 100, 75, 80, 83, 71, 62, 56, 5...\n",
              " Name: ra, Length: 393656, dtype: object]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "id": "POEyvHfcIBws",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba093b8e-b605-4697-cf08-9f03aec26a3f"
      },
      "source": [
        "try:\n",
        "  del train\n",
        "except Exception as e:\n",
        "  print(\"train already deleted\")\n",
        "gc.collect(), gc.collect()"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60, 0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzzYXM3bba7p"
      },
      "source": [
        "# Create train/val datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJMvNzqIY8QS"
      },
      "source": [
        "# Lists of numpy arrays\r\n",
        "train_list = [features.values[:train_len] for features in feature_lists]\r\n",
        "val_list = [features.values[train_len:] for features in feature_lists]"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wkQrxFMtI-s"
      },
      "source": [
        "def select_window_size(*x):\n",
        "    series_len = len(x[0]) # Length of series\n",
        "    if series_len <= THR_E: # Just return the sequence!\n",
        "        return x\n",
        "    else: # Random select from sequence\n",
        "        max_select = series_len - THR_E\n",
        "        random_select = tf.random.uniform(shape=(), minval=0, maxval=max_select, dtype=tf.int32)\n",
        "        x = [i[random_select:random_select+THR_E] for i in x]\n",
        "        x = tuple(x)\n",
        "        return x"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "teDfc5v2UNAp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b48cb4e1-a303-470a-ab17-43f9a5b59a91"
      },
      "source": [
        "train_list"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([list([5692, 5716, 128, 7860, 7922, 156, 51, 50, 7896, 7863, 152, 104, 108, 7900, 7901, 7971, 25, 183, 7926, 7927, 4, 7984, 45, 185, 55, 7876, 6, 172, 7898, 175, 100, 7859, 57, 7948, 151, 167, 7897, 7882, 7962, 1278, 2065, 2064, 2063, 3363, 3365, 3364]),\n",
              "        list([7900, 7876, 175, 1278, 2064, 2063, 2065, 3364, 3365, 3363, 2948, 2947, 2946, 2595, 2593, 2594, 4492, 4120, 4696, 6116, 6173, 6370, 6909, 6910, 6908, 6911, 7218, 7216, 7217, 7219]),\n",
              "        list([5273, 758, 5976, 236, 404, 382, 405, 873, 531, 775, 294, 714, 297, 297, 775, 1295, 10684, 1014, 484]),\n",
              "        ...,\n",
              "        list([7900, 7876, 175, 1278, 2064, 2063, 2065, 3364, 3365, 3363, 2947, 2948, 2946, 2593, 2595, 2594, 4492, 4120, 4696, 6116, 6173, 6370, 6878, 6880, 6879, 6877, 7219, 7218, 7217, 7216, 6106, 4755, 9313, 3586, 4476, 6432, 5845, 9094, 6191, 5437]),\n",
              "        list([7900, 7876, 175, 1278, 2064, 2063, 2065, 3364, 3363, 3365, 2946, 2948, 2947, 2594, 2595, 2593, 4492, 4120, 4696, 6116, 6173, 6370, 6879, 6880, 6877, 6878, 7219, 7218, 7216, 7217]),\n",
              "        list([7900, 7876, 175, 1278, 2065, 2064, 2063, 3363, 3364, 3365, 2947, 2948, 2946, 2593, 2594, 2595, 4492, 4120, 4696, 6116, 6173, 6370, 6878, 6877, 6880, 6879, 7217, 7218, 7216, 7219])],\n",
              "       dtype=object),\n",
              " array([list([3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1]),\n",
              "        list([3, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]),\n",
              "        list([3, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1]),\n",
              "        ...,\n",
              "        list([3, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0]),\n",
              "        list([3, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1]),\n",
              "        list([3, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0])],\n",
              "       dtype=object),\n",
              " array([list([301, 37, 55, 19, 11, 5, 17, 17, 16, 16, 17, 22, 23, 21, 24, 22, 21, 20, 18, 17, 29, 20, 19, 21, 22, 16, 20, 22, 22, 23, 20, 15, 20, 20, 22, 19, 17, 20, 17, 21, 17, 0, 0, 14, 0, 0]),\n",
              "        list([301, 26, 29, 26, 18, 0, 0, 33, 0, 0, 21, 0, 0, 22, 0, 0, 15, 32, 23, 27, 14, 17, 7, 0, 0, 0, 6, 0, 0, 0]),\n",
              "        list([301, 28, 17, 24, 20, 16, 16, 19, 18, 18, 20, 13, 13, 16, 15, 16, 17, 19, 20]),\n",
              "        ...,\n",
              "        list([301, 19, 28, 25, 13, 0, 0, 22, 0, 0, 12, 0, 0, 2, 0, 0, 1, 44, 13, 68, 25, 48, 21, 0, 0, 0, 36, 0, 0, 0, 60, 88, 18, 34, 10, 13, 19, 5, 11, 9]),\n",
              "        list([301, 27, 20, 13, 16, 0, 0, 47, 0, 0, 19, 0, 0, 14, 0, 0, 19, 26, 15, 18, 17, 30, 16, 0, 0, 0, 24, 0, 0, 0]),\n",
              "        list([301, 18, 18, 20, 17, 0, 0, 22, 0, 0, 17, 0, 0, 15, 0, 0, 24, 25, 21, 9, 7, 10, 11, 0, 0, 0, 26, 0, 0, 0])],\n",
              "       dtype=object),\n",
              " array([list([173, 49, 53, 5, 1, 11, 11, 10, 10, 11, 16, 17, 15, 18, 16, 15, 14, 12, 11, 23, 14, 12, 16, 15, 10, 15, 16, 16, 17, 14, 9, 14, 141, 25, 23, 29, 31, 48, 45, 170, 95, 0, 0, 99, 0, 0]),\n",
              "        list([173, 25, 21, 14, 92, 0, 0, 61, 0, 0, 64, 0, 0, 44, 0, 0, 28, 18, 22, 9, 13, 2, 23, 0, 0, 0, 9, 0, 0, 0]),\n",
              "        list([173, 14, 19, 15, 12, 12, 15, 88, 41, 37, 21, 19, 36, 128, 22, 23, 19, 30, 27]),\n",
              "        ...,\n",
              "        list([173, 24, 19, 8, 61, 0, 0, 32, 0, 0, 1, 0, 0, 0, 0, 0, 39, 8, 63, 20, 43, 16, 110, 0, 0, 0, 128, 0, 0, 0, 171, 22, 33, 9, 16, 21, 4, 9, 8, 16]),\n",
              "        list([173, 16, 8, 11, 109, 0, 0, 50, 0, 0, 36, 0, 0, 52, 0, 0, 21, 10, 13, 12, 25, 11, 86, 0, 0, 0, 108, 0, 0, 0]),\n",
              "        list([173, 13, 15, 11, 60, 0, 0, 46, 0, 0, 39, 0, 0, 68, 0, 0, 20, 16, 3, 1, 4, 6, 92, 0, 0, 0, 118, 0, 0, 0])],\n",
              "       dtype=object),\n",
              " array([list([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              "        list([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              "        list([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0]),\n",
              "        ...,\n",
              "        list([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              "        list([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              "        list([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])],\n",
              "       dtype=object),\n",
              " array([list([101, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 91, 83, 77, 79, 80, 81, 82, 78, 79, 80, 76, 73, 70, 67, 68, 69, 70, 71, 72, 70, 71, 69, 70, 71, 71, 72, 70, 71, 72, 72, 73, 74, 72, 70, 69]),\n",
              "        list([101, 100, 50, 67, 50, 40, 33, 43, 38, 33, 30, 27, 33, 31, 36, 33, 31, 29, 28, 26, 30, 29, 27, 30, 29, 28, 27, 26, 25, 24]),\n",
              "        list([101, 0, 0, 0, 25, 20, 33, 43, 50, 56, 50, 55, 58, 54, 57, 53, 56, 59, 56]),\n",
              "        ...,\n",
              "        list([101, 100, 50, 33, 50, 40, 33, 29, 25, 22, 30, 27, 25, 23, 21, 20, 25, 24, 22, 21, 20, 19, 18, 22, 25, 24, 23, 22, 25, 28, 30, 29, 31, 33, 35, 34, 36, 38, 39, 41]),\n",
              "        list([101, 0, 0, 0, 0, 20, 33, 29, 38, 33, 30, 27, 33, 31, 29, 27, 25, 29, 28, 26, 25, 24, 23, 26, 29, 28, 31, 30, 29, 28]),\n",
              "        list([101, 100, 50, 33, 25, 20, 33, 29, 25, 33, 30, 27, 25, 23, 21, 27, 25, 29, 28, 32, 30, 29, 32, 35, 38, 36, 35, 33, 36, 38])],\n",
              "       dtype=object)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_uGV6iLARz1"
      },
      "source": [
        "def create_train_dataset(N_training_per_epoch, probs_train):\n",
        "  index = np.random.choice(train_len, N_training_per_epoch, replace=True, p = probs_train) # random indexing (similar to shuffling)\n",
        "  tr_list = [features[index] for features in train_list] # Selecting by index \n",
        "  train_ds = (tf.data.Dataset\n",
        "                  .from_generator(lambda: iter(zip(*tr_list)), output_types=OUTPUT_TYPES)\n",
        "                  .map(select_window_size)\n",
        "                  .padded_batch(batch_size = BATCH_SIZE, padded_shapes = PADDED_SHAPES, padding_values = PADDING_VALUES)\n",
        "                  .prefetch(AUTO)\n",
        "  )\n",
        "  return train_ds"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyI8fDf7Ik4A"
      },
      "source": [
        "def create_val_dataset(N_valid_per_epoch, probs_val):\n",
        "  index = np.random.choice(val_len, N_valid_per_epoch, replace=True, p = probs_val) # random indexing (similar to shuffling)\n",
        "  vl_list = [features[index] for features in val_list] # Selecting by index \n",
        "  val_ds = (tf.data.Dataset\n",
        "                  .from_generator(lambda: iter(zip(*vl_list)), output_types=OUTPUT_TYPES)\n",
        "                  .map(select_window_size)\n",
        "                  .padded_batch(batch_size = BATCH_SIZE, padded_shapes = PADDED_SHAPES, padding_values = PADDING_VALUES)\n",
        "                  .cache()\n",
        "                  .prefetch(AUTO)\n",
        "  )\n",
        "  return val_ds"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDJudy20sEdg"
      },
      "source": [
        "val_dataset = create_val_dataset(N_SELECT_PER_EPOCH, PROBS_VAL)\r\n",
        "a = iter(val_dataset) # Small check"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMsYgAgVJhxq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3e87330-d63e-4a41-a4c6-74a4e3cf2060"
      },
      "source": [
        "next(a)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(128, 100), dtype=int32, numpy=\n",
              " array([[ 1726,  1725,  3115, ...,  2250,  2249,  2122],\n",
              "        [ 5257,  4405,  4240, ...,  4414,  2946,  2947],\n",
              "        [   54,    34,   124, ...,  1525,  1523,  2544],\n",
              "        ...,\n",
              "        [  208,  1118,   937, ...,  1386,   254,   587],\n",
              "        [ 5195,  5621,   454, ...,  4108,   756,  1223],\n",
              "        [ 8249,   249,  6099, ..., 13523, 13523, 13523]], dtype=int32)>,\n",
              " <tf.Tensor: shape=(128, 101), dtype=int32, numpy=\n",
              " array([[1, 1, 1, ..., 1, 1, 2],\n",
              "        [1, 1, 1, ..., 1, 0, 2],\n",
              "        [1, 1, 1, ..., 1, 0, 2],\n",
              "        ...,\n",
              "        [0, 1, 1, ..., 1, 1, 2],\n",
              "        [0, 1, 1, ..., 1, 1, 2],\n",
              "        [3, 0, 1, ..., 2, 2, 2]], dtype=int32)>,\n",
              " <tf.Tensor: shape=(128, 100), dtype=int32, numpy=\n",
              " array([[  0,   0,  22, ...,   0,   0,  26],\n",
              "        [ 24,  22,  10, ...,  13,   4,   0],\n",
              "        [ 16,  23,  23, ...,   0,   0,   4],\n",
              "        ...,\n",
              "        [ 22,  19,  20, ...,  17,  17,  18],\n",
              "        [  9,  10,  11, ...,  13,   6,  18],\n",
              "        [301,  53,  18, ..., 302, 302, 302]], dtype=int32)>,\n",
              " <tf.Tensor: shape=(128, 100), dtype=int32, numpy=\n",
              " array([[  0,   0,  68, ...,   0,   0,  86],\n",
              "        [ 21,  10,  17, ...,   2,  73,   0],\n",
              "        [ 21,  21,  23, ...,   0,   0,  76],\n",
              "        ...,\n",
              "        [ 34,  37,  38, ...,  29,  36, 169],\n",
              "        [  8,   8,   8, ..., 154,  56,  38],\n",
              "        [173,  38,   2, ..., 174, 174, 174]], dtype=int32)>,\n",
              " <tf.Tensor: shape=(128, 100), dtype=int32, numpy=\n",
              " array([[1, 1, 1, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 1, 1, 1],\n",
              "        [2, 2, 2, ..., 2, 2, 1],\n",
              "        ...,\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 1, 0, 0],\n",
              "        [0, 0, 0, ..., 8, 8, 8]], dtype=int32)>,\n",
              " <tf.Tensor: shape=(128, 100), dtype=int32, numpy=\n",
              " array([[ 79,  79,  79, ...,  80,  80,  80],\n",
              "        [ 67,  68,  68, ...,  71,  71,  71],\n",
              "        [ 76,  76,  76, ...,  76,  76,  76],\n",
              "        ...,\n",
              "        [ 66,  66,  66, ...,  66,  66,  66],\n",
              "        [ 64,  67,  69, ...,  84,  84,  85],\n",
              "        [101,   0,  50, ..., 102, 102, 102]], dtype=int32)>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yz9Fk6sp6sgP"
      },
      "source": [
        "## Get model and set hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "feqp6ENL6sgT"
      },
      "source": [
        "%run $MODEL_FILE"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEQBB9KthwmS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a158ffab-fbf8-4533-d642-834b5dd24603"
      },
      "source": [
        "shutil.copy('/content/drive/My Drive/Colab Notebooks/demo-riiid-transformer.ipynb', OUTPUT_FOLDER)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'29-Dec-riiid-1/demo-riiid-transformer.ipynb'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9IBAuXw6sgY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8612691f-9960-47d2-847d-14c12641c542"
      },
      "source": [
        "# Hyperparameters main model\n",
        "config = {\"features\": selections, \"n_features\": len(selections), \"vocab_sizes\": vocab_sizes, \n",
        "          \"enc_emb\": ENC_EMB, \"enc_dense\": ENC_DENSE, \"dec_emb\": DEC_EMB, \"dec_dense\": DEC_DENSE, \n",
        "          \"window_size\": THR_E, \"enc_num_layers\": 2, \"dec_num_layers\": 2, \"d_model\": 256, \n",
        "          \"dff\": 512, \"num_heads\": 8, \"dropout_rate\": 0.1, \"pos_encoding\": True,\n",
        "          \"padding_values\": PADDING_VALUES, \"output_types\": OUTPUT_TYPES, \"padded_shapes\": PADDED_SHAPES, \n",
        "          \"quantile_transformer_et\": quantile_transformer_et,  \"quantile_transformer_lt\": quantile_transformer_lt, \"ltg_bins\": ltg_bins, \"zero_task_etc\": zero_task_etc}\n",
        "\n",
        "with open(os.path.join(OUTPUT_FOLDER, \"config.pickle\"), 'wb') as handle:\n",
        "  pickle.dump(config, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "logging.info('config: %s', config)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "29-12 18:30 root         INFO     config: {'features': ['E', 'r', 'etc', 'ltg', 'at', 'ra'], 'n_features': 6, 'vocab_sizes': [13524, 4, 303, 175, 9, 103], 'enc_emb': [0, 4], 'enc_dense': [], 'dec_emb': [1, 2, 3, 5], 'dec_dense': [], 'window_size': 100, 'enc_num_layers': 2, 'dec_num_layers': 2, 'd_model': 256, 'dff': 512, 'num_heads': 8, 'dropout_rate': 0.1, 'pos_encoding': True, 'padding_values': (13523, 2, 302, 174, 8, 102), 'output_types': (tf.int32, tf.int32, tf.int32, tf.int32, tf.int32, tf.int32), 'padded_shapes': ([100], [101], [100], [100], [100], [100]), 'quantile_transformer_et': [], 'quantile_transformer_lt': [], 'ltg_bins': array([0.00000000e+00, 6.00000000e+00, 9.00000000e+00, 1.00000000e+01,\n",
            "       1.10000000e+01, 1.20000000e+01, 1.30000000e+01, 1.40000000e+01,\n",
            "       1.50000000e+01, 1.60000000e+01, 1.70000000e+01, 1.80000000e+01,\n",
            "       1.90000000e+01, 2.00000000e+01, 2.10000000e+01, 2.20000000e+01,\n",
            "       2.30000000e+01, 2.40000000e+01, 2.50000000e+01, 2.60000000e+01,\n",
            "       2.70000000e+01, 2.80000000e+01, 2.90000000e+01, 3.00000000e+01,\n",
            "       3.10000000e+01, 3.20000000e+01, 3.30000000e+01, 3.40000000e+01,\n",
            "       3.50000000e+01, 3.60000000e+01, 3.70000000e+01, 3.80000000e+01,\n",
            "       3.90000000e+01, 4.00000000e+01, 4.10000000e+01, 4.20000000e+01,\n",
            "       4.30000000e+01, 4.40000000e+01, 4.50000000e+01, 4.60000000e+01,\n",
            "       4.70000000e+01, 4.80000000e+01, 4.90000000e+01, 5.00000000e+01,\n",
            "       5.10000000e+01, 5.20000000e+01, 5.30000000e+01, 5.40000000e+01,\n",
            "       5.50000000e+01, 5.60000000e+01, 5.70000000e+01, 5.80000000e+01,\n",
            "       5.90000000e+01, 6.00000000e+01, 6.10000000e+01, 6.20000000e+01,\n",
            "       6.30000000e+01, 6.40000000e+01, 6.50000000e+01, 6.60000000e+01,\n",
            "       6.70000000e+01, 6.80000000e+01, 6.90000000e+01, 7.00000000e+01,\n",
            "       7.10000000e+01, 7.20000000e+01, 7.30000000e+01, 7.40000000e+01,\n",
            "       7.50000000e+01, 7.60000000e+01, 7.70000000e+01, 7.80000000e+01,\n",
            "       7.90000000e+01, 8.00000000e+01, 8.10000000e+01, 8.20000000e+01,\n",
            "       8.30000000e+01, 8.40000000e+01, 8.50000000e+01, 8.70000000e+01,\n",
            "       8.80000000e+01, 8.90000000e+01, 9.00000000e+01, 9.10000000e+01,\n",
            "       9.30000000e+01, 9.40000000e+01, 9.50000000e+01, 9.70000000e+01,\n",
            "       9.80000000e+01, 1.00000000e+02, 1.01000000e+02, 1.03000000e+02,\n",
            "       1.04000000e+02, 1.06000000e+02, 1.08000000e+02, 1.09000000e+02,\n",
            "       1.11000000e+02, 1.13000000e+02, 1.15000000e+02, 1.17000000e+02,\n",
            "       1.19000000e+02, 1.21000000e+02, 1.24000000e+02, 1.26000000e+02,\n",
            "       1.28000000e+02, 1.31000000e+02, 1.34000000e+02, 1.36000000e+02,\n",
            "       1.39000000e+02, 1.42000000e+02, 1.45000000e+02, 1.48000000e+02,\n",
            "       1.52000000e+02, 1.55000000e+02, 1.59000000e+02, 1.63000000e+02,\n",
            "       1.67000000e+02, 1.71000000e+02, 1.75000000e+02, 1.80000000e+02,\n",
            "       1.85000000e+02, 1.90000000e+02, 1.96000000e+02, 2.02000000e+02,\n",
            "       2.08000000e+02, 2.15000000e+02, 2.23000000e+02, 2.30000000e+02,\n",
            "       2.39000000e+02, 2.48000000e+02, 2.58000000e+02, 2.69000000e+02,\n",
            "       2.81000000e+02, 2.94000000e+02, 3.08000000e+02, 3.24000000e+02,\n",
            "       3.42000000e+02, 3.62000000e+02, 3.84000000e+02, 4.11000000e+02,\n",
            "       4.41000000e+02, 4.78000000e+02, 5.21000000e+02, 5.75000000e+02,\n",
            "       6.42000000e+02, 7.28000000e+02, 8.42000000e+02, 9.95000000e+02,\n",
            "       1.20600000e+03, 1.50552750e+03, 1.94200000e+03, 2.58800000e+03,\n",
            "       3.53900000e+03, 4.92400000e+03, 6.90500000e+03, 9.67100000e+03,\n",
            "       1.35800000e+04, 1.85920000e+04, 2.51710000e+04, 3.27690000e+04,\n",
            "       4.03800000e+04, 4.81590000e+04, 5.95090000e+04, 7.16370000e+04,\n",
            "       8.14450000e+04, 8.66470000e+04, 9.90910000e+04, 1.31473000e+05,\n",
            "       1.73794000e+05, 2.58621000e+05, 4.39883785e+05, 1.16173171e+06,\n",
            "       8.38842610e+07]), 'zero_task_etc': True}\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gIEWr8V6sgf"
      },
      "source": [
        "transformer = create_model_separate_input(config)\n",
        "# tf.keras.utils.plot_model(transformer, os.path.join(OUTPUT_FOLDER, \"model_plot.png\"), show_shapes=True)"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DiW6F6Rv66fx"
      },
      "source": [
        "## Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWAf5VTW66fz"
      },
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "  def __init__(self, d_model, warmup_steps=4000):\n",
        "    super(CustomSchedule, self).__init__()\n",
        "    \n",
        "    self.d_model = d_model\n",
        "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "    self.warmup_steps = warmup_steps\n",
        "  def __call__(self, step):\n",
        "    arg1 = tf.math.rsqrt(step)\n",
        "    arg2 = step * (self.warmup_steps ** -1.5)\n",
        "    \n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYjhA2aa66f2"
      },
      "source": [
        "learning_rate = CustomSchedule(config[\"d_model\"])\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.999, \n",
        "                                      epsilon=1e-9)"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVVIhusN66f5"
      },
      "source": [
        "## Loss and metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdEZ2qRz66f6"
      },
      "source": [
        "Since the target sequences are padded, it is important to apply a padding mask when calculating the loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNgoEGX666f6"
      },
      "source": [
        "# Loss and metric\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "accuracy_object = tf.keras.metrics.sparse_categorical_accuracy\n",
        "\n",
        "train_auc= tf.keras.metrics.AUC()\n",
        "val_auc= tf.keras.metrics.AUC()"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_Bm57bt66f9"
      },
      "source": [
        "def loss_function(real, pred): # batch_size x seq_size x 1 vs  batch_size x seq_size x 3\n",
        "    mask = tf.math.logical_not(tf.math.equal(tf.squeeze(real), N_response)) # batch_size x seq_size\n",
        "    loss_ = loss_object(real, pred) # batch_size x seq_size\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    \n",
        "    loss_ *= mask \n",
        "    loss_ = tf.reduce_sum(loss_)/tf.reduce_sum(mask) #loss becomes one value! (from all batches)\n",
        "    return loss_"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahEaU3kN66f_"
      },
      "source": [
        "def metric_function(real, pred, auc_object): # batch_size x seq_size x 1 vs  batch_size x seq_size x 3\n",
        "    # Predict accuracy\n",
        "    mask = tf.math.logical_not(tf.math.equal(tf.squeeze(real), N_response)) # batch_size x seq_size\n",
        "    accuracy = accuracy_object(real, pred) # batch_size x seq_size\n",
        "    mask = tf.cast(mask, dtype=accuracy.dtype)\n",
        "    \n",
        "    accuracy *= mask\n",
        "    accuracy = 100*tf.reduce_sum(accuracy)/tf.reduce_sum(mask)\n",
        "    \n",
        "    # A work-around to predict AUC => is it stable?\n",
        "    pred = tf.nn.softmax(pred)\n",
        "    pred = pred[:,:,1] # pred that answer is correct\n",
        "    real = tf.keras.backend.flatten(real)\n",
        "    pred = tf.keras.backend.flatten(pred)\n",
        "    \n",
        "    idxs = tf.math.logical_not(tf.math.equal(real, N_response))\n",
        "    real = real[idxs]\n",
        "    pred = pred[idxs]\n",
        "    auc = auc_object(real, pred)\n",
        "    return accuracy"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cm9x2sLp66gC"
      },
      "source": [
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')\n",
        "\n",
        "val_loss = tf.keras.metrics.Mean(name='val_loss')\n",
        "val_accuracy = tf.keras.metrics.Mean(name='val_accuracy')"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeHumfr7zmMa"
      },
      "source": [
        "## Training and checkpointing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fzuf06YZp66w"
      },
      "source": [
        "Create the checkpoint path and the checkpoint manager. This will be used to save checkpoints every `n` epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNhuYfllndLZ"
      },
      "source": [
        "checkpoint_path = os.path.join(OUTPUT_FOLDER, \"checkpoints/\")\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
        "                           optimizer=optimizer)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=1)\n",
        "\n",
        "# https://stackoverflow.com/questions/62919208/how-to-restore-a-specific-checkpoint-in-tensorflow2-to-implement-early-stopping\n",
        "\n",
        "# # if a checkpoint exists, restore the latest checkpoint.\n",
        "# if ckpt_manager.latest_checkpoint:\n",
        "#   ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "#   print ('Latest checkpoint restored!!')"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxjpegZH_311"
      },
      "source": [
        "# train_step_signature = [\r\n",
        "#     tf.TensorSpec(shape=(None, THR_E), dtype=tf.int32),\r\n",
        "#     tf.TensorSpec(shape=(None, THR_E), dtype=tf.int32),\r\n",
        "#     tf.TensorSpec(shape=(None, THR_E), dtype=tf.int32),\r\n",
        "#     tf.TensorSpec(shape=(None, THR_E), dtype=tf.int32),\r\n",
        "#     tf.TensorSpec(shape=(None, THR_E), dtype=tf.int32),\r\n",
        "#     tf.TensorSpec(shape=(None, THR_E), dtype=tf.int32),\r\n",
        "#     tf.TensorSpec(shape=(None, THR_E), dtype=tf.int32),\r\n",
        "# ]"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nsLZJ6FznKF"
      },
      "source": [
        "@tf.function()\n",
        "def train_step(tar_real, *inputs):\n",
        "    \n",
        "    with tf.GradientTape() as tape: \n",
        "        predictions = transformer(inputs, training=True)\n",
        "        tar_real = tf.expand_dims(tar_real, -1) # IMPORTANT! DOESN'T WORK WITHOUT IT. ALWAYS GIVES ERROR INCOMPATIBLE SHAPE. E.G. (32,169) vs. (32,169,3)\n",
        "        \n",
        "        loss = loss_function(tar_real, predictions)\n",
        "        accuracy = metric_function(tar_real, predictions, auc_object = train_auc)\n",
        "    \n",
        "    gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
        "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "\n",
        "    train_loss(loss)\n",
        "    train_accuracy(accuracy)\n",
        "\n",
        "@tf.function()\n",
        "def val_step(tar_real, *inputs):\n",
        "    predictions = transformer(inputs, training=False)\n",
        "    tar_real = tf.expand_dims(tar_real, -1) # IMPORTANT! DOESN'T WORK WITHOUT IT. ALWAYS GIVES ERROR INCOMPATIBLE SHAPE. E.G. (32,169) vs. (32,169,3)\n",
        "    \n",
        "    loss = loss_function(tar_real, predictions)\n",
        "    accuracy = metric_function(tar_real, predictions, auc_object = val_auc)\n",
        "\n",
        "    val_loss(loss)\n",
        "    val_accuracy(accuracy)"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbvmaKNiznHZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf222843-264b-4ba4-cba3-c1422500343b"
      },
      "source": [
        "best_auc = 0\n",
        "for epoch in range(EPOCHS):\n",
        "    train_dataset = create_train_dataset(N_SELECT_PER_EPOCH, PROBS_TRAIN) # create train for each epoch\n",
        "    start = time.time()\n",
        "\n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "    train_auc.reset_states()\n",
        "    \n",
        "    val_loss.reset_states()\n",
        "    val_accuracy.reset_states()\n",
        "    val_auc.reset_states()\n",
        "\n",
        "    # Train\n",
        "    for (batch, (features)) in enumerate(train_dataset):\n",
        "        E, tar, rest = features[0], features[1], features[2:] \n",
        "        tar_inp = tar[:, :-1]\n",
        "        tar_real = tar[:, 1:]\n",
        "\n",
        "        inputs = [E, tar_inp] + list(rest)        \n",
        "        train_step(tar_real, inputs)\n",
        "        \n",
        "        if batch % PRINT_EVERY_N_BATCHES == 0:\n",
        "            print ('Epoch {} TRAIN Batch {} Loss {:.4f} Accuracy {:.4f} AUC {:.4f}'.format(\n",
        "              epoch + 1, batch, train_loss.result(), train_accuracy.result(), train_auc.result()))\n",
        "\n",
        "    logging.info('Epoch TRAIN {} Loss {:.4f} Accuracy {:.4f} AUC {:.4f}'.format(epoch + 1, \n",
        "                                                train_loss.result(), \n",
        "                                                train_accuracy.result(),\n",
        "                                                train_auc.result()\n",
        "                                                ))\n",
        "    \n",
        "    # Validate and save model\n",
        "    if (epoch + 1) % VAL_EVERY_N_EPOCHS == 0: \n",
        "      for (batch, (features)) in enumerate(val_dataset):\n",
        "          E, tar, rest = features[0], features[1], features[2:] \n",
        "          tar_inp = tar[:, :-1]\n",
        "          tar_real = tar[:, 1:]\n",
        "\n",
        "          inputs = [E, tar_inp] + list(rest)          \n",
        "          val_step(tar_real, inputs)\n",
        "\n",
        "          if batch % PRINT_EVERY_N_BATCHES == 0:\n",
        "              print ('Epoch {} VAL Batch {} Loss {:.4f} Accuracy {:.4f} AUC {:.4f}'.format(\n",
        "                epoch + 1, batch, val_loss.result(), val_accuracy.result(), val_auc.result()))\n",
        "      \n",
        "      logging.info('Epoch VAL {} Loss {:.4f} Accuracy {:.4f} AUC {:.4f}'.format(epoch + 1, \n",
        "                                          val_loss.result(), \n",
        "                                          val_accuracy.result(),\n",
        "                                          val_auc.result()\n",
        "                                          ))    \n",
        "       \n",
        "      if val_auc.result() > best_auc:\n",
        "          best_auc = val_auc.result()\n",
        "          ckpt_save_path = ckpt_manager.save()\n",
        "          logging.info('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n",
        "                                                            ckpt_save_path))\n",
        "        \n",
        "    print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 TRAIN Batch 0 Loss 1.4967 Accuracy 20.5670 AUC 0.5070\n",
            "Epoch 1 TRAIN Batch 50 Loss 0.9553 Accuracy 51.6246 AUC 0.5023\n",
            "Epoch 1 TRAIN Batch 100 Loss 0.8247 Accuracy 56.8592 AUC 0.5018\n",
            "Epoch 1 TRAIN Batch 150 Loss 0.7742 Accuracy 59.1481 AUC 0.5029\n",
            "Epoch 1 TRAIN Batch 200 Loss 0.7459 Accuracy 60.5054 AUC 0.5044\n",
            "Epoch 1 TRAIN Batch 250 Loss 0.7270 Accuracy 61.5125 AUC 0.5064\n",
            "Epoch 1 TRAIN Batch 300 Loss 0.7132 Accuracy 62.2572 AUC 0.5094\n",
            "Epoch 1 TRAIN Batch 350 Loss 0.7025 Accuracy 62.8574 AUC 0.5137\n",
            "Epoch 1 TRAIN Batch 400 Loss 0.6935 Accuracy 63.3138 AUC 0.5207\n",
            "Epoch 1 TRAIN Batch 450 Loss 0.6859 Accuracy 63.6857 AUC 0.5288\n",
            "Epoch 1 TRAIN Batch 500 Loss 0.6793 Accuracy 64.0464 AUC 0.5361\n",
            "Epoch 1 TRAIN Batch 550 Loss 0.6740 Accuracy 64.3311 AUC 0.5420\n",
            "Epoch 1 TRAIN Batch 600 Loss 0.6693 Accuracy 64.5741 AUC 0.5477\n",
            "Epoch 1 TRAIN Batch 650 Loss 0.6652 Accuracy 64.8014 AUC 0.5526\n",
            "Epoch 1 TRAIN Batch 700 Loss 0.6615 Accuracy 65.0126 AUC 0.5574\n",
            "Epoch 1 TRAIN Batch 750 Loss 0.6584 Accuracy 65.1776 AUC 0.5615\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "29-12 18:32 root         INFO     Epoch TRAIN 1 Loss 0.6565 Accuracy 65.2813 AUC 0.5639\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Time taken for 1 epoch: 157.54356837272644 secs\n",
            "\n",
            "Epoch 2 TRAIN Batch 0 Loss 0.6167 Accuracy 67.1481 AUC 0.6298\n",
            "Epoch 2 TRAIN Batch 50 Loss 0.6113 Accuracy 67.6672 AUC 0.6302\n",
            "Epoch 2 TRAIN Batch 100 Loss 0.6103 Accuracy 67.7147 AUC 0.6334\n",
            "Epoch 2 TRAIN Batch 150 Loss 0.6096 Accuracy 67.7917 AUC 0.6335\n",
            "Epoch 2 TRAIN Batch 200 Loss 0.6086 Accuracy 67.8490 AUC 0.6351\n",
            "Epoch 2 TRAIN Batch 250 Loss 0.6080 Accuracy 67.8392 AUC 0.6368\n",
            "Epoch 2 TRAIN Batch 300 Loss 0.6074 Accuracy 67.8460 AUC 0.6384\n",
            "Epoch 2 TRAIN Batch 350 Loss 0.6069 Accuracy 67.8558 AUC 0.6393\n",
            "Epoch 2 TRAIN Batch 400 Loss 0.6064 Accuracy 67.8579 AUC 0.6404\n",
            "Epoch 2 TRAIN Batch 450 Loss 0.6059 Accuracy 67.8831 AUC 0.6411\n",
            "Epoch 2 TRAIN Batch 500 Loss 0.6057 Accuracy 67.8782 AUC 0.6418\n",
            "Epoch 2 TRAIN Batch 550 Loss 0.6052 Accuracy 67.8917 AUC 0.6431\n",
            "Epoch 2 TRAIN Batch 600 Loss 0.6046 Accuracy 67.9091 AUC 0.6442\n",
            "Epoch 2 TRAIN Batch 650 Loss 0.6043 Accuracy 67.9317 AUC 0.6449\n",
            "Epoch 2 TRAIN Batch 700 Loss 0.6039 Accuracy 67.9392 AUC 0.6459\n",
            "Epoch 2 TRAIN Batch 750 Loss 0.6035 Accuracy 67.9541 AUC 0.6469\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "29-12 18:35 root         INFO     Epoch TRAIN 2 Loss 0.6032 Accuracy 67.9691 AUC 0.6474\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Time taken for 1 epoch: 151.29352068901062 secs\n",
            "\n",
            "Epoch 3 TRAIN Batch 0 Loss 0.5990 Accuracy 67.5816 AUC 0.6791\n",
            "Epoch 3 TRAIN Batch 50 Loss 0.5963 Accuracy 68.2894 AUC 0.6667\n",
            "Epoch 3 TRAIN Batch 100 Loss 0.5845 Accuracy 69.2022 AUC 0.6891\n",
            "Epoch 3 TRAIN Batch 150 Loss 0.5710 Accuracy 70.2766 AUC 0.7112\n",
            "Epoch 3 TRAIN Batch 200 Loss 0.5634 Accuracy 70.8532 AUC 0.7229\n",
            "Epoch 3 TRAIN Batch 250 Loss 0.5583 Accuracy 71.2279 AUC 0.7306\n",
            "Epoch 3 TRAIN Batch 300 Loss 0.5540 Accuracy 71.5423 AUC 0.7365\n",
            "Epoch 3 TRAIN Batch 350 Loss 0.5515 Accuracy 71.7176 AUC 0.7401\n",
            "Epoch 3 TRAIN Batch 400 Loss 0.5491 Accuracy 71.8816 AUC 0.7431\n",
            "Epoch 3 TRAIN Batch 450 Loss 0.5470 Accuracy 72.0199 AUC 0.7458\n",
            "Epoch 3 TRAIN Batch 500 Loss 0.5453 Accuracy 72.1382 AUC 0.7480\n",
            "Epoch 3 TRAIN Batch 550 Loss 0.5442 Accuracy 72.2057 AUC 0.7496\n",
            "Epoch 3 TRAIN Batch 600 Loss 0.5430 Accuracy 72.2959 AUC 0.7511\n",
            "Epoch 3 TRAIN Batch 650 Loss 0.5421 Accuracy 72.3502 AUC 0.7522\n",
            "Epoch 3 TRAIN Batch 700 Loss 0.5412 Accuracy 72.4163 AUC 0.7533\n",
            "Epoch 3 TRAIN Batch 750 Loss 0.5403 Accuracy 72.4810 AUC 0.7543\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "29-12 18:37 root         INFO     Epoch TRAIN 3 Loss 0.5399 Accuracy 72.5104 AUC 0.7548\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Time taken for 1 epoch: 150.39379000663757 secs\n",
            "\n",
            "Epoch 4 TRAIN Batch 0 Loss 0.5178 Accuracy 74.1474 AUC 0.7655\n",
            "Epoch 4 TRAIN Batch 50 Loss 0.5301 Accuracy 73.1268 AUC 0.7660\n",
            "Epoch 4 TRAIN Batch 100 Loss 0.5292 Accuracy 73.2018 AUC 0.7672\n",
            "Epoch 4 TRAIN Batch 150 Loss 0.5294 Accuracy 73.2234 AUC 0.7672\n",
            "Epoch 4 TRAIN Batch 200 Loss 0.5292 Accuracy 73.2426 AUC 0.7676\n",
            "Epoch 4 TRAIN Batch 250 Loss 0.5291 Accuracy 73.2470 AUC 0.7677\n",
            "Epoch 4 TRAIN Batch 300 Loss 0.5288 Accuracy 73.2624 AUC 0.7681\n",
            "Epoch 4 TRAIN Batch 350 Loss 0.5288 Accuracy 73.2556 AUC 0.7682\n",
            "Epoch 4 TRAIN Batch 400 Loss 0.5286 Accuracy 73.2717 AUC 0.7685\n",
            "Epoch 4 TRAIN Batch 450 Loss 0.5284 Accuracy 73.2814 AUC 0.7686\n",
            "Epoch 4 TRAIN Batch 500 Loss 0.5285 Accuracy 73.2689 AUC 0.7687\n",
            "Epoch 4 TRAIN Batch 550 Loss 0.5285 Accuracy 73.2756 AUC 0.7688\n",
            "Epoch 4 TRAIN Batch 600 Loss 0.5283 Accuracy 73.2865 AUC 0.7689\n",
            "Epoch 4 TRAIN Batch 650 Loss 0.5282 Accuracy 73.2942 AUC 0.7690\n",
            "Epoch 4 TRAIN Batch 700 Loss 0.5280 Accuracy 73.3099 AUC 0.7691\n",
            "Epoch 4 TRAIN Batch 750 Loss 0.5279 Accuracy 73.3154 AUC 0.7692\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "29-12 18:40 root         INFO     Epoch TRAIN 4 Loss 0.5280 Accuracy 73.3145 AUC 0.7692\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Time taken for 1 epoch: 154.8648931980133 secs\n",
            "\n",
            "Epoch 5 TRAIN Batch 0 Loss 0.5281 Accuracy 73.5089 AUC 0.7727\n",
            "Epoch 5 TRAIN Batch 50 Loss 0.5282 Accuracy 73.2543 AUC 0.7690\n",
            "Epoch 5 TRAIN Batch 100 Loss 0.5270 Accuracy 73.3625 AUC 0.7696\n",
            "Epoch 5 TRAIN Batch 150 Loss 0.5268 Accuracy 73.3952 AUC 0.7701\n",
            "Epoch 5 TRAIN Batch 200 Loss 0.5269 Accuracy 73.3946 AUC 0.7700\n",
            "Epoch 5 TRAIN Batch 250 Loss 0.5271 Accuracy 73.3625 AUC 0.7699\n",
            "Epoch 5 TRAIN Batch 300 Loss 0.5267 Accuracy 73.3943 AUC 0.7702\n",
            "Epoch 5 TRAIN Batch 350 Loss 0.5267 Accuracy 73.3991 AUC 0.7703\n",
            "Epoch 5 TRAIN Batch 400 Loss 0.5267 Accuracy 73.3968 AUC 0.7703\n",
            "Epoch 5 TRAIN Batch 450 Loss 0.5266 Accuracy 73.4068 AUC 0.7706\n",
            "Epoch 5 TRAIN Batch 500 Loss 0.5264 Accuracy 73.4218 AUC 0.7706\n",
            "Epoch 5 TRAIN Batch 550 Loss 0.5264 Accuracy 73.4210 AUC 0.7707\n",
            "Epoch 5 TRAIN Batch 600 Loss 0.5264 Accuracy 73.4155 AUC 0.7707\n",
            "Epoch 5 TRAIN Batch 650 Loss 0.5263 Accuracy 73.4255 AUC 0.7709\n",
            "Epoch 5 TRAIN Batch 700 Loss 0.5261 Accuracy 73.4438 AUC 0.7711\n",
            "Epoch 5 TRAIN Batch 750 Loss 0.5258 Accuracy 73.4585 AUC 0.7713\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "29-12 18:42 root         INFO     Epoch TRAIN 5 Loss 0.5259 Accuracy 73.4570 AUC 0.7713\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 5 VAL Batch 0 Loss 0.5201 Accuracy 73.6955 AUC 0.7725\n",
            "Epoch 5 VAL Batch 50 Loss 0.5195 Accuracy 73.8914 AUC 0.7760\n",
            "Epoch 5 VAL Batch 100 Loss 0.5205 Accuracy 73.7977 AUC 0.7754\n",
            "Epoch 5 VAL Batch 150 Loss 0.5202 Accuracy 73.8286 AUC 0.7755\n",
            "Epoch 5 VAL Batch 200 Loss 0.5205 Accuracy 73.7813 AUC 0.7754\n",
            "Epoch 5 VAL Batch 250 Loss 0.5202 Accuracy 73.8173 AUC 0.7757\n",
            "Epoch 5 VAL Batch 300 Loss 0.5204 Accuracy 73.7989 AUC 0.7756\n",
            "Epoch 5 VAL Batch 350 Loss 0.5204 Accuracy 73.8037 AUC 0.7755\n",
            "Epoch 5 VAL Batch 400 Loss 0.5205 Accuracy 73.7952 AUC 0.7755\n",
            "Epoch 5 VAL Batch 450 Loss 0.5205 Accuracy 73.8008 AUC 0.7757\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4vnYx13H1gX"
      },
      "source": [
        "from IPython.display import FileLink, FileLinks\n",
        "FileLinks(OUTPUT_FOLDER) #lists all downloadable files on server"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_rWpnRgyiyC"
      },
      "source": [
        "## Add output to Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ph8Un94HsahC"
      },
      "source": [
        "OUTPUT_DRIVE = \"/content/drive/My Drive/kaggle-riiid/subs\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eglhQhuZsqyw"
      },
      "source": [
        "# https://stackoverflow.com/questions/15034151/copy-directory-contents-into-a-directory-with-python\n",
        "from distutils.dir_util import copy_tree\n",
        "copy_tree(OUTPUT_FOLDER, os.path.join(OUTPUT_DRIVE, OUTPUT_FOLDER))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Lzwa2GGzqAT"
      },
      "source": [
        "## Add output to Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOu1E5Xop-57"
      },
      "source": [
        "KAGGLE_JSON = \"/content/drive/My\\ Drive/kaggle-riiid/kaggle.json\"\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp $KAGGLE_JSON ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0N2cmXLygls"
      },
      "source": [
        "!kaggle datasets init -p {OUTPUT_FOLDER}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbCedVa7z5M-"
      },
      "source": [
        "import json\n",
        "\n",
        "with open(f'{OUTPUT_FOLDER}/dataset-metadata.json', 'r+') as f:\n",
        "    data = json.load(f)\n",
        "    data['title'] = OUTPUT_FOLDER\n",
        "    data['id'] = f'rafiko1/{OUTPUT_FOLDER}'\n",
        "    f.seek(0)\n",
        "    json.dump(data, f, indent=4)\n",
        "    f.truncate()\n",
        "\n",
        "!cat {OUTPUT_FOLDER}/dataset-metadata.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QiYmcZ0r0hoh"
      },
      "source": [
        "!kaggle datasets create -p {OUTPUT_FOLDER} -q -r zip "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfFKlQG5KL09"
      },
      "source": [
        "os.listdir(OUTPUT_FOLDER)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rfa2BsoE29sJ"
      },
      "source": [
        "ltg_bins"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqQ1fIsLwkGE"
      },
      "source": [
        "## Summary\n",
        "\n",
        "In this tutorial, you learned about positional encoding, multi-head attention, the importance of masking and how to create a transformer.\n",
        "\n",
        "Try using a different dataset to train the transformer. You can also create the base transformer or transformer XL by changing the hyperparameters above. You can also use the layers defined here to create [BERT](https://arxiv.org/abs/1810.04805) and train state of the art models. Futhermore, you can implement beam search to get better predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ob_U1TjRE6A-"
      },
      "source": [
        "# Older code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCPtV7asE7w_"
      },
      "source": [
        "# # From https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly\n",
        "# class DataGenerator(keras.utils.Sequence):\n",
        "#     'Generates data for Keras'\n",
        "#     def __init__(self, df, shuffle=True):\n",
        "#         'Initialization'\n",
        "        \n",
        "#         self.df = df\n",
        "#         self.users = self.df[\"user_id\"].nunique()\n",
        "#         self.batch_size = BATCH_SIZE\n",
        "#         self.shuffle = shuffle\n",
        "#         self.on_epoch_end()\n",
        "\n",
        "#     def __len__(self):\n",
        "#         'Denotes the number of batches per epoch'\n",
        "#         return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
        "\n",
        "#     def __getitem__(self, index):\n",
        "#         'Generate one batch of data'\n",
        "#         # Generate indexes of the batch\n",
        "#         indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "\n",
        "#         # Find list of IDs\n",
        "#         list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
        "\n",
        "#         # Generate data\n",
        "#         X, y = self.__data_generation(list_IDs_temp)\n",
        "\n",
        "#         return X, y\n",
        "\n",
        "#     def on_epoch_end(self):\n",
        "#         'Updates indexes after each epoch'\n",
        "#         self.indexes = np.arange(len(self.list_IDs))\n",
        "#         if self.shuffle == True:\n",
        "#             np.random.shuffle(self.indexes)\n",
        "\n",
        "#     def __data_generation(self, list_IDs_temp):\n",
        "#         'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
        "#         # Initialization\n",
        "#         X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
        "#         y = np.empty((self.batch_size), dtype=int)\n",
        "\n",
        "#         # Generate data\n",
        "#         for i, ID in enumerate(list_IDs_temp):\n",
        "#             # Store sample\n",
        "#             X[i,] = np.load('data/' + ID + '.npy')\n",
        "\n",
        "#             # Store class\n",
        "#             y[i] = self.labels[ID]\n",
        "\n",
        "#         return X, keras.utils.to_categorical(y, num_classes=self.n_classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UrylowI_ThEY"
      },
      "source": [
        "# def save_dict(seq_dict, filename):\n",
        "#     with open(os.path.join(OUTPUT_FOLDER, filename), 'wb') as handle:\n",
        "#       pickle.dump(seq_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTwScXwCThEY"
      },
      "source": [
        "# # Save last N dict to memory\n",
        "# E_dict = E_lists.apply(lambda x: x[-200:]).to_dict()\n",
        "# r_dict = r_lists.apply(lambda x: x[-200:]).to_dict()\n",
        "# et_dict = et_lists.apply(lambda x: x[-200:]).to_dict()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbXwrK5eThEY"
      },
      "source": [
        "# save_dict(E_dict, \"E.pkl\")\n",
        "# save_dict(r_dict, \"r.pkl\")\n",
        "# save_dict(et_dict, \"et.pkl\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}